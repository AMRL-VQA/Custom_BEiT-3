{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not all images have caption annotations\n",
      "82783 82774 82774\n",
      "Write ./dataset/vqa.train.jsonl with 434867 items !\n",
      "not all images have caption annotations\n",
      "40504 40503 40503\n",
      "Write ./dataset/vqa.val.jsonl with 210051 items !\n",
      "all images have caption annotations\n",
      "81434 81434 81434\n",
      "Write ./dataset/vqa.test.jsonl with 447793 items !\n",
      "not all images have caption annotations\n",
      "81434 36807 36807\n",
      "Write ./dataset/vqa.test-dev.jsonl with 107394 items !\n",
      "Contains 40503 image and 210051 pairs for val set!\n",
      "Write ./dataset/vqa.trainable_val.jsonl with 204645 items !\n",
      "Write ./dataset/vqa.rest_val.jsonl with 5406 items !\n"
     ]
    }
   ],
   "source": [
    "from datasets import VQAv2Dataset\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer(\"./model//beit3.spm\")\n",
    "\n",
    "VQAv2Dataset.make_dataset_index(\n",
    "    data_path=\"./dataset/\",\n",
    "    tokenizer=tokenizer,\n",
    "    annotation_data_path=\"./dataset/vqa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Codes\\python_projects_windows\\VQA\\BEiT-3\\unilm\\beit3\\run_beit3_finetuning.py\", line 448, in <module>\n",
      "    main(opts, ds_init)\n",
      "  File \"d:\\Codes\\python_projects_windows\\VQA\\BEiT-3\\unilm\\beit3\\run_beit3_finetuning.py\", line 365, in main\n",
      "    utils.dump_predictions(args, result, \"vqav2_test\")\n",
      "  File \"d:\\Codes\\python_projects_windows\\VQA\\BEiT-3\\unilm\\beit3\\utils.py\", line 868, in dump_predictions\n",
      "    torch.distributed.barrier()\n",
      "  File \"c:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 3327, in barrier\n",
      "    default_pg = _get_default_group()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 707, in _get_default_group\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Namespace(model='beit3_large_patch16_480', task='vqav2', input_size=480, drop_path=0.1, checkpoint_activations=None, sentencepiece_model='./model/beit3.spm', vocab_size=64010, num_max_bpe_tokens=64, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, lr=0.0005, layer_decay=0.9, task_head_lr_weight=0, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, batch_size=16, eval_batch_size=None, epochs=20, update_freq=1, save_ckpt_freq=5, randaug=False, train_interpolation='bicubic', finetune='./model/beit3_large_indomain_patch16_768_vgqaaug_vqa.zip', model_key='model|module', model_prefix='', data_path='./dataset/', output_dir='./output/', log_dir=None, device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=True, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', task_cache_path='./output/', nb_classes=1000, mixup=0, cutmix=0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, captioning_mask_prob=0.6, drop_worst_ratio=0.2, drop_worst_after=12000, num_beams=3, length_penalty=0.6, label_smoothing=0.1, enable_deepspeed=False, initial_scale_power=16, zero_stage=0, distributed=False)\n",
      "Load 434867 image-text pairs from ./dataset/vqa.train.jsonl. \n",
      "Load 204645 image-text pairs from ./dataset/vqa.trainable_val.jsonl. \n",
      "Load 5406 image-text pairs from ./dataset/vqa.rest_val.jsonl. \n",
      "model_config = beit3_large_patch16_480_vqav2\n",
      "Load ckpt from ./model/beit3_large_indomain_patch16_768_vgqaaug_vqa.zip\n",
      "Load state_dict by model_key = model\n",
      "Position interpolate from 48x48 to 30x30\n",
      "Model = BEiT3ForVisualQuestionAnswering(\n",
      "  (beit3): BEiT3(\n",
      "    (text_embed): TextEmbedding(64010, 1024)\n",
      "    (vision_embed): VisionEmbedding(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "      (embed_positions): MutliwayEmbedding(\n",
      "        (A): PositionalEmbedding(903, 1024)\n",
      "        (B): PositionalEmbedding(1024, 1024)\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.0)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.004347826086956522)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.008695652173913044)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.013043478260869566)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.017391304347826087)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.021739130434782608)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (6): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.026086956521739132)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (7): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.030434782608695653)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (8): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.034782608695652174)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (9): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.0391304347826087)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (10): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.043478260869565216)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (11): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.04782608695652174)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (12): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.052173913043478265)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (13): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.05652173913043478)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (14): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.06086956521739131)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (15): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.06521739130434782)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (16): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.06956521739130435)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (17): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.07391304347826087)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (18): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.0782608695652174)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (19): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.08260869565217391)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (20): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.08695652173913043)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (21): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.09130434782608696)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (22): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.09565217391304348)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (23): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.1)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Linear(in_features=2048, out_features=3129, bias=True)\n",
      "  )\n",
      ")\n",
      "number of params: 682985529\n",
      "LR = 0.00050000\n",
      "Batch size = 16\n",
      "Update frequent = 1\n",
      "Number of training examples = 639512\n",
      "Number of training training per epoch = 39969\n",
      "Assigned values = [0.0717897987691853, 0.07976644307687256, 0.08862938119652507, 0.09847709021836118, 0.10941898913151242, 0.12157665459056935, 0.13508517176729928, 0.15009463529699918, 0.16677181699666577, 0.18530201888518416, 0.20589113209464907, 0.2287679245496101, 0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]\n",
      "Param groups = {\n",
      "  \"layer_0_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.text_embed.weight\",\n",
      "      \"beit3.vision_embed.mask_token\",\n",
      "      \"beit3.vision_embed.proj.weight\",\n",
      "      \"beit3.encoder.embed_positions.B.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.0717897987691853\n",
      "  },\n",
      "  \"layer_0_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.vision_embed.cls_token\",\n",
      "      \"beit3.vision_embed.proj.bias\",\n",
      "      \"beit3.encoder.embed_positions.A.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.0717897987691853\n",
      "  },\n",
      "  \"layer_1_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.07976644307687256\n",
      "  },\n",
      "  \"layer_1_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.07976644307687256\n",
      "  },\n",
      "  \"layer_2_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.08862938119652507\n",
      "  },\n",
      "  \"layer_2_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.08862938119652507\n",
      "  },\n",
      "  \"layer_3_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.09847709021836118\n",
      "  },\n",
      "  \"layer_3_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.09847709021836118\n",
      "  },\n",
      "  \"layer_4_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.10941898913151242\n",
      "  },\n",
      "  \"layer_4_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.10941898913151242\n",
      "  },\n",
      "  \"layer_5_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.12157665459056935\n",
      "  },\n",
      "  \"layer_5_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.12157665459056935\n",
      "  },\n",
      "  \"layer_6_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.13508517176729928\n",
      "  },\n",
      "  \"layer_6_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.13508517176729928\n",
      "  },\n",
      "  \"layer_7_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.15009463529699918\n",
      "  },\n",
      "  \"layer_7_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.15009463529699918\n",
      "  },\n",
      "  \"layer_8_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.16677181699666577\n",
      "  },\n",
      "  \"layer_8_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.16677181699666577\n",
      "  },\n",
      "  \"layer_9_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.18530201888518416\n",
      "  },\n",
      "  \"layer_9_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.18530201888518416\n",
      "  },\n",
      "  \"layer_10_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.20589113209464907\n",
      "  },\n",
      "  \"layer_10_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.20589113209464907\n",
      "  },\n",
      "  \"layer_11_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2287679245496101\n",
      "  },\n",
      "  \"layer_11_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2287679245496101\n",
      "  },\n",
      "  \"layer_12_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2541865828329001\n",
      "  },\n",
      "  \"layer_12_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2541865828329001\n",
      "  },\n",
      "  \"layer_13_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.12.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2824295364810001\n",
      "  },\n",
      "  \"layer_13_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.12.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.12.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.12.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.12.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.12.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2824295364810001\n",
      "  },\n",
      "  \"layer_14_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.13.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31381059609000006\n",
      "  },\n",
      "  \"layer_14_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.13.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.13.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.13.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.13.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.13.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31381059609000006\n",
      "  },\n",
      "  \"layer_15_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.14.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3486784401000001\n",
      "  },\n",
      "  \"layer_15_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.14.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.14.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.14.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.14.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.14.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3486784401000001\n",
      "  },\n",
      "  \"layer_16_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.15.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3874204890000001\n",
      "  },\n",
      "  \"layer_16_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.15.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.15.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.15.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.15.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.15.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3874204890000001\n",
      "  },\n",
      "  \"layer_17_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.16.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4304672100000001\n",
      "  },\n",
      "  \"layer_17_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.16.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.16.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.16.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.16.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.16.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4304672100000001\n",
      "  },\n",
      "  \"layer_18_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.17.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4782969000000001\n",
      "  },\n",
      "  \"layer_18_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.17.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.17.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.17.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.17.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.17.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4782969000000001\n",
      "  },\n",
      "  \"layer_19_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.18.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.531441\n",
      "  },\n",
      "  \"layer_19_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.18.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.18.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.18.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.18.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.18.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.531441\n",
      "  },\n",
      "  \"layer_20_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.19.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5904900000000001\n",
      "  },\n",
      "  \"layer_20_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.19.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.19.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.19.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.19.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.19.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5904900000000001\n",
      "  },\n",
      "  \"layer_21_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.20.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.6561\n",
      "  },\n",
      "  \"layer_21_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.20.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.20.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.20.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.20.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.20.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.6561\n",
      "  },\n",
      "  \"layer_22_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.21.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.7290000000000001\n",
      "  },\n",
      "  \"layer_22_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.21.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.21.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.21.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.21.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.21.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.7290000000000001\n",
      "  },\n",
      "  \"layer_23_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.22.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.81\n",
      "  },\n",
      "  \"layer_23_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.22.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.22.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.22.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.22.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.22.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.81\n",
      "  },\n",
      "  \"layer_24_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.23.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.9\n",
      "  },\n",
      "  \"layer_24_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.23.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.23.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.23.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.23.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.23.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.9\n",
      "  },\n",
      "  \"layer_25_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"pooler.norm.weight\",\n",
      "      \"pooler.norm.bias\",\n",
      "      \"pooler.dense.bias\",\n",
      "      \"head.0.bias\",\n",
      "      \"head.1.weight\",\n",
      "      \"head.1.bias\",\n",
      "      \"head.3.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"layer_25_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"pooler.dense.weight\",\n",
      "      \"head.0.weight\",\n",
      "      \"head.3.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "Set warmup steps = 199845\n",
      "Auto resume checkpoint: \n",
      "Load 447793 image-text pairs from ./dataset/vqa.test.jsonl. \n",
      "Test:  [    0/18659]  eta: 12 days, 18:08:58    time: 59.0674  data: 56.5850  max mem: 7129\n",
      "Test:  [   10/18659]  eta: 1 day, 9:52:30    time: 6.5392  data: 5.1441  max mem: 7129\n",
      "Test:  [   20/18659]  eta: 20:54:52    time: 1.2881  data: 0.0000  max mem: 7129\n",
      "Test:  [   30/18659]  eta: 16:18:41    time: 1.2892  data: 0.0000  max mem: 7129\n",
      "Test:  [   40/18659]  eta: 13:58:13    time: 1.2960  data: 0.0001  max mem: 7129\n",
      "Test:  [   50/18659]  eta: 12:32:04    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [   60/18659]  eta: 11:34:02    time: 1.2914  data: 0.0000  max mem: 7129\n",
      "Test:  [   70/18659]  eta: 10:53:29    time: 1.3045  data: 0.0001  max mem: 7129\n",
      "Test:  [   80/18659]  eta: 10:22:06    time: 1.3080  data: 0.0001  max mem: 7129\n",
      "Test:  [   90/18659]  eta: 9:57:19    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [  100/18659]  eta: 9:37:23    time: 1.2900  data: 0.0000  max mem: 7129\n",
      "Test:  [  110/18659]  eta: 9:21:02    time: 1.2903  data: 0.0000  max mem: 7129\n",
      "Test:  [  120/18659]  eta: 9:07:28    time: 1.2931  data: 0.0001  max mem: 7129\n",
      "Test:  [  130/18659]  eta: 8:55:50    time: 1.2932  data: 0.0002  max mem: 7129\n",
      "Test:  [  140/18659]  eta: 8:45:58    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [  150/18659]  eta: 8:37:14    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [  160/18659]  eta: 8:29:35    time: 1.2912  data: 0.0002  max mem: 7129\n",
      "Test:  [  170/18659]  eta: 8:22:47    time: 1.2910  data: 0.0002  max mem: 7129\n",
      "Test:  [  180/18659]  eta: 8:16:42    time: 1.2908  data: 0.0001  max mem: 7129\n",
      "Test:  [  190/18659]  eta: 8:11:15    time: 1.2909  data: 0.0001  max mem: 7129\n",
      "Test:  [  200/18659]  eta: 8:06:19    time: 1.2910  data: 0.0002  max mem: 7129\n",
      "Test:  [  210/18659]  eta: 8:01:50    time: 1.2911  data: 0.0002  max mem: 7129\n",
      "Test:  [  220/18659]  eta: 7:57:45    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [  230/18659]  eta: 7:54:30    time: 1.3107  data: 0.0002  max mem: 7129\n",
      "Test:  [  240/18659]  eta: 7:51:09    time: 1.3158  data: 0.0001  max mem: 7129\n",
      "Test:  [  250/18659]  eta: 7:47:55    time: 1.2968  data: 0.0001  max mem: 7129\n",
      "Test:  [  260/18659]  eta: 7:44:55    time: 1.2913  data: 0.0001  max mem: 7129\n",
      "Test:  [  270/18659]  eta: 7:42:07    time: 1.2911  data: 0.0001  max mem: 7129\n",
      "Test:  [  280/18659]  eta: 7:39:30    time: 1.2912  data: 0.0002  max mem: 7129\n",
      "Test:  [  290/18659]  eta: 7:37:03    time: 1.2913  data: 0.0002  max mem: 7129\n",
      "Test:  [  300/18659]  eta: 7:34:46    time: 1.2915  data: 0.0001  max mem: 7129\n",
      "Test:  [  310/18659]  eta: 7:32:36    time: 1.2917  data: 0.0000  max mem: 7129\n",
      "Test:  [  320/18659]  eta: 7:30:34    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [  330/18659]  eta: 7:28:39    time: 1.2925  data: 0.0001  max mem: 7129\n",
      "Test:  [  340/18659]  eta: 7:26:49    time: 1.2921  data: 0.0002  max mem: 7129\n",
      "Test:  [  350/18659]  eta: 7:25:05    time: 1.2921  data: 0.0002  max mem: 7129\n",
      "Test:  [  360/18659]  eta: 7:23:26    time: 1.2923  data: 0.0001  max mem: 7129\n",
      "Test:  [  370/18659]  eta: 7:21:52    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [  380/18659]  eta: 7:20:21    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [  390/18659]  eta: 7:18:55    time: 1.2916  data: 0.0002  max mem: 7129\n",
      "Test:  [  400/18659]  eta: 7:17:32    time: 1.2919  data: 0.0002  max mem: 7129\n",
      "Test:  [  410/18659]  eta: 7:16:13    time: 1.2920  data: 0.0002  max mem: 7129\n",
      "Test:  [  420/18659]  eta: 7:14:57    time: 1.2919  data: 0.0001  max mem: 7129\n",
      "Test:  [  430/18659]  eta: 7:13:44    time: 1.2920  data: 0.0001  max mem: 7129\n",
      "Test:  [  440/18659]  eta: 7:12:34    time: 1.2919  data: 0.0000  max mem: 7129\n",
      "Test:  [  450/18659]  eta: 7:11:26    time: 1.2918  data: 0.0002  max mem: 7129\n",
      "Test:  [  460/18659]  eta: 7:10:21    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [  470/18659]  eta: 7:09:17    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [  480/18659]  eta: 7:08:16    time: 1.2916  data: 0.0002  max mem: 7129\n",
      "Test:  [  490/18659]  eta: 7:07:17    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [  500/18659]  eta: 7:06:20    time: 1.2919  data: 0.0000  max mem: 7129\n",
      "Test:  [  510/18659]  eta: 7:05:24    time: 1.2925  data: 0.0001  max mem: 7129\n",
      "Test:  [  520/18659]  eta: 7:04:30    time: 1.2923  data: 0.0001  max mem: 7129\n",
      "Test:  [  530/18659]  eta: 7:03:38    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [  540/18659]  eta: 7:02:47    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [  550/18659]  eta: 7:01:57    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [  560/18659]  eta: 7:01:21    time: 1.3095  data: 0.0000  max mem: 7129\n",
      "Test:  [  570/18659]  eta: 7:00:33    time: 1.3095  data: 0.0000  max mem: 7129\n",
      "Test:  [  580/18659]  eta: 6:59:51    time: 1.2976  data: 0.0000  max mem: 7129\n",
      "Test:  [  590/18659]  eta: 6:59:06    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [  600/18659]  eta: 6:58:22    time: 1.2924  data: 0.0002  max mem: 7129\n",
      "Test:  [  610/18659]  eta: 6:57:39    time: 1.2924  data: 0.0001  max mem: 7129\n",
      "Test:  [  620/18659]  eta: 6:56:57    time: 1.2920  data: 0.0001  max mem: 7129\n",
      "Test:  [  630/18659]  eta: 6:56:16    time: 1.2919  data: 0.0002  max mem: 7129\n",
      "Test:  [  640/18659]  eta: 6:55:36    time: 1.2917  data: 0.0002  max mem: 7129\n",
      "Test:  [  650/18659]  eta: 6:54:57    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [  660/18659]  eta: 6:54:18    time: 1.2919  data: 0.0001  max mem: 7129\n",
      "Test:  [  670/18659]  eta: 6:53:41    time: 1.2920  data: 0.0003  max mem: 7129\n",
      "Test:  [  680/18659]  eta: 6:53:04    time: 1.2919  data: 0.0002  max mem: 7129\n",
      "Test:  [  690/18659]  eta: 6:52:28    time: 1.2922  data: 0.0001  max mem: 7129\n",
      "Test:  [  700/18659]  eta: 6:51:52    time: 1.2922  data: 0.0001  max mem: 7129\n",
      "Test:  [  710/18659]  eta: 6:51:17    time: 1.2921  data: 0.0002  max mem: 7129\n",
      "Test:  [  720/18659]  eta: 6:50:43    time: 1.2922  data: 0.0002  max mem: 7129\n",
      "Test:  [  730/18659]  eta: 6:50:09    time: 1.2922  data: 0.0001  max mem: 7129\n",
      "Test:  [  740/18659]  eta: 6:49:36    time: 1.2923  data: 0.0001  max mem: 7129\n",
      "Test:  [  750/18659]  eta: 6:49:03    time: 1.2925  data: 0.0000  max mem: 7129\n",
      "Test:  [  760/18659]  eta: 6:48:31    time: 1.2927  data: 0.0001  max mem: 7129\n",
      "Test:  [  770/18659]  eta: 6:48:00    time: 1.2928  data: 0.0001  max mem: 7129\n",
      "Test:  [  780/18659]  eta: 6:47:29    time: 1.2931  data: 0.0000  max mem: 7129\n",
      "Test:  [  790/18659]  eta: 6:46:59    time: 1.2935  data: 0.0000  max mem: 7129\n",
      "Test:  [  800/18659]  eta: 6:46:28    time: 1.2932  data: 0.0001  max mem: 7129\n",
      "Test:  [  810/18659]  eta: 6:46:07    time: 1.3117  data: 0.0003  max mem: 7129\n",
      "Test:  [  820/18659]  eta: 6:45:38    time: 1.3116  data: 0.0002  max mem: 7129\n",
      "Test:  [  830/18659]  eta: 6:45:09    time: 1.2924  data: 0.0003  max mem: 7129\n",
      "Test:  [  840/18659]  eta: 6:44:45    time: 1.3038  data: 0.0002  max mem: 7129\n",
      "Test:  [  850/18659]  eta: 6:44:16    time: 1.3038  data: 0.0001  max mem: 7129\n",
      "Test:  [  860/18659]  eta: 6:43:48    time: 1.2923  data: 0.0001  max mem: 7129\n",
      "Test:  [  870/18659]  eta: 6:43:21    time: 1.2927  data: 0.0000  max mem: 7129\n",
      "Test:  [  880/18659]  eta: 6:42:54    time: 1.2931  data: 0.0000  max mem: 7129\n",
      "Test:  [  890/18659]  eta: 6:42:27    time: 1.2927  data: 0.0001  max mem: 7129\n",
      "Test:  [  900/18659]  eta: 6:42:00    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [  910/18659]  eta: 6:41:33    time: 1.2923  data: 0.0002  max mem: 7129\n",
      "Test:  [  920/18659]  eta: 6:41:07    time: 1.2920  data: 0.0001  max mem: 7129\n",
      "Test:  [  930/18659]  eta: 6:40:41    time: 1.2920  data: 0.0001  max mem: 7129\n",
      "Test:  [  940/18659]  eta: 6:40:16    time: 1.2922  data: 0.0001  max mem: 7129\n",
      "Test:  [  950/18659]  eta: 6:39:50    time: 1.2923  data: 0.0004  max mem: 7129\n",
      "Test:  [  960/18659]  eta: 6:39:25    time: 1.2924  data: 0.0003  max mem: 7129\n",
      "Test:  [  970/18659]  eta: 6:39:01    time: 1.2925  data: 0.0001  max mem: 7129\n",
      "Test:  [  980/18659]  eta: 6:38:36    time: 1.2926  data: 0.0002  max mem: 7129\n",
      "Test:  [  990/18659]  eta: 6:38:12    time: 1.2926  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1000/18659]  eta: 6:37:51    time: 1.3011  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1010/18659]  eta: 6:37:29    time: 1.3066  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1020/18659]  eta: 6:37:06    time: 1.3015  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1030/18659]  eta: 6:36:43    time: 1.2960  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1040/18659]  eta: 6:36:20    time: 1.2924  data: 0.0003  max mem: 7129\n",
      "Test:  [ 1050/18659]  eta: 6:35:57    time: 1.2924  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1060/18659]  eta: 6:35:34    time: 1.2925  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1070/18659]  eta: 6:35:11    time: 1.2928  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1080/18659]  eta: 6:34:49    time: 1.2926  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1090/18659]  eta: 6:34:26    time: 1.2919  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1100/18659]  eta: 6:34:04    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1110/18659]  eta: 6:33:42    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1120/18659]  eta: 6:33:20    time: 1.2919  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1130/18659]  eta: 6:32:58    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1140/18659]  eta: 6:32:36    time: 1.2917  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1150/18659]  eta: 6:32:15    time: 1.2921  data: 0.0003  max mem: 7129\n",
      "Test:  [ 1160/18659]  eta: 6:31:54    time: 1.2924  data: 0.0003  max mem: 7129\n",
      "Test:  [ 1170/18659]  eta: 6:31:33    time: 1.2922  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1180/18659]  eta: 6:31:11    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1190/18659]  eta: 6:30:51    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1200/18659]  eta: 6:30:30    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1210/18659]  eta: 6:30:09    time: 1.2916  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1220/18659]  eta: 6:29:48    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1230/18659]  eta: 6:29:28    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1240/18659]  eta: 6:29:08    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1250/18659]  eta: 6:28:49    time: 1.2957  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1260/18659]  eta: 6:28:29    time: 1.2955  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1270/18659]  eta: 6:28:09    time: 1.2919  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1280/18659]  eta: 6:27:49    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1290/18659]  eta: 6:27:29    time: 1.2915  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1300/18659]  eta: 6:27:09    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1310/18659]  eta: 6:26:50    time: 1.2918  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1320/18659]  eta: 6:26:30    time: 1.2916  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1330/18659]  eta: 6:26:11    time: 1.2921  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1340/18659]  eta: 6:25:52    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1350/18659]  eta: 6:25:33    time: 1.2925  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1360/18659]  eta: 6:25:14    time: 1.2921  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1370/18659]  eta: 6:24:55    time: 1.2920  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1380/18659]  eta: 6:24:36    time: 1.2919  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1390/18659]  eta: 6:24:17    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1400/18659]  eta: 6:23:59    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1410/18659]  eta: 6:23:40    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1420/18659]  eta: 6:23:22    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1430/18659]  eta: 6:23:03    time: 1.2922  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1440/18659]  eta: 6:22:45    time: 1.2922  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1450/18659]  eta: 6:22:26    time: 1.2916  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1460/18659]  eta: 6:22:08    time: 1.2916  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1470/18659]  eta: 6:21:50    time: 1.2917  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1480/18659]  eta: 6:21:33    time: 1.2969  data: 0.0003  max mem: 7129\n",
      "Test:  [ 1490/18659]  eta: 6:21:16    time: 1.3008  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1500/18659]  eta: 6:20:58    time: 1.2954  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1510/18659]  eta: 6:20:40    time: 1.2916  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1520/18659]  eta: 6:20:22    time: 1.2918  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1530/18659]  eta: 6:20:04    time: 1.2918  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1540/18659]  eta: 6:19:47    time: 1.2918  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1550/18659]  eta: 6:19:29    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1560/18659]  eta: 6:19:12    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1570/18659]  eta: 6:18:55    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1580/18659]  eta: 6:18:40    time: 1.3028  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1590/18659]  eta: 6:18:23    time: 1.3049  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1600/18659]  eta: 6:18:05    time: 1.2936  data: 0.0003  max mem: 7129\n",
      "Test:  [ 1610/18659]  eta: 6:17:48    time: 1.2919  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1620/18659]  eta: 6:17:31    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1630/18659]  eta: 6:17:14    time: 1.2926  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1640/18659]  eta: 6:16:58    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1650/18659]  eta: 6:16:41    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1660/18659]  eta: 6:16:24    time: 1.2964  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1670/18659]  eta: 6:16:07    time: 1.2965  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1680/18659]  eta: 6:15:51    time: 1.2928  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1690/18659]  eta: 6:15:34    time: 1.2961  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1700/18659]  eta: 6:15:18    time: 1.2994  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1710/18659]  eta: 6:15:02    time: 1.2965  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1720/18659]  eta: 6:14:45    time: 1.2932  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1730/18659]  eta: 6:14:28    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1740/18659]  eta: 6:14:12    time: 1.2924  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1750/18659]  eta: 6:13:55    time: 1.2923  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1760/18659]  eta: 6:13:38    time: 1.2920  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1770/18659]  eta: 6:13:22    time: 1.2920  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1780/18659]  eta: 6:13:05    time: 1.2921  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1790/18659]  eta: 6:12:49    time: 1.2924  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1800/18659]  eta: 6:12:32    time: 1.2925  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1810/18659]  eta: 6:12:16    time: 1.2924  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1820/18659]  eta: 6:12:00    time: 1.2925  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1830/18659]  eta: 6:11:43    time: 1.2927  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1840/18659]  eta: 6:11:27    time: 1.2927  data: 0.0003  max mem: 7129\n",
      "Test:  [ 1850/18659]  eta: 6:11:11    time: 1.2927  data: 0.0003  max mem: 7129\n",
      "Test:  [ 1860/18659]  eta: 6:10:55    time: 1.2959  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1870/18659]  eta: 6:10:39    time: 1.2961  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1880/18659]  eta: 6:10:23    time: 1.2927  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1890/18659]  eta: 6:10:07    time: 1.2929  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1900/18659]  eta: 6:09:52    time: 1.2967  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1910/18659]  eta: 6:09:36    time: 1.2964  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1920/18659]  eta: 6:09:21    time: 1.2975  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1930/18659]  eta: 6:09:05    time: 1.3006  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1940/18659]  eta: 6:08:49    time: 1.2955  data: 0.0002  max mem: 7129\n",
      "Test:  [ 1950/18659]  eta: 6:08:33    time: 1.2922  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1960/18659]  eta: 6:08:17    time: 1.2923  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1970/18659]  eta: 6:08:02    time: 1.2922  data: 0.0001  max mem: 7129\n",
      "Test:  [ 1980/18659]  eta: 6:07:46    time: 1.2923  data: 0.0000  max mem: 7129\n",
      "Test:  [ 1990/18659]  eta: 6:07:30    time: 1.2925  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2000/18659]  eta: 6:07:14    time: 1.2925  data: 0.0003  max mem: 7129\n",
      "Test:  [ 2010/18659]  eta: 6:06:58    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2020/18659]  eta: 6:06:43    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2030/18659]  eta: 6:06:27    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2040/18659]  eta: 6:06:11    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2050/18659]  eta: 6:05:56    time: 1.2920  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2060/18659]  eta: 6:05:40    time: 1.2922  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2070/18659]  eta: 6:05:24    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2080/18659]  eta: 6:05:09    time: 1.2920  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2090/18659]  eta: 6:04:53    time: 1.2924  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2100/18659]  eta: 6:04:38    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2110/18659]  eta: 6:04:22    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2120/18659]  eta: 6:04:07    time: 1.2919  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2130/18659]  eta: 6:03:51    time: 1.2922  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2140/18659]  eta: 6:03:36    time: 1.2923  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2150/18659]  eta: 6:03:20    time: 1.2920  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2160/18659]  eta: 6:03:05    time: 1.2920  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2170/18659]  eta: 6:02:50    time: 1.2924  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2180/18659]  eta: 6:02:35    time: 1.2929  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2190/18659]  eta: 6:02:19    time: 1.2928  data: 0.0003  max mem: 7129\n",
      "Test:  [ 2200/18659]  eta: 6:02:04    time: 1.2924  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2210/18659]  eta: 6:01:49    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2220/18659]  eta: 6:01:36    time: 1.3086  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2230/18659]  eta: 6:01:21    time: 1.3084  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2240/18659]  eta: 6:01:05    time: 1.2919  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2250/18659]  eta: 6:00:50    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2260/18659]  eta: 6:00:35    time: 1.2921  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2270/18659]  eta: 6:00:20    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2280/18659]  eta: 6:00:05    time: 1.2929  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2290/18659]  eta: 5:59:50    time: 1.2924  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2300/18659]  eta: 5:59:35    time: 1.2920  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2310/18659]  eta: 5:59:20    time: 1.2918  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2320/18659]  eta: 5:59:04    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2330/18659]  eta: 5:58:50    time: 1.2959  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2340/18659]  eta: 5:58:35    time: 1.2959  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2350/18659]  eta: 5:58:20    time: 1.2916  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2360/18659]  eta: 5:58:05    time: 1.2915  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2370/18659]  eta: 5:57:50    time: 1.2916  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2380/18659]  eta: 5:57:36    time: 1.3017  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2390/18659]  eta: 5:57:23    time: 1.3130  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2400/18659]  eta: 5:57:08    time: 1.3069  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2410/18659]  eta: 5:56:53    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2420/18659]  eta: 5:56:39    time: 1.2987  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2430/18659]  eta: 5:56:24    time: 1.2988  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2440/18659]  eta: 5:56:10    time: 1.2915  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2450/18659]  eta: 5:55:55    time: 1.2920  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2460/18659]  eta: 5:55:40    time: 1.2924  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2470/18659]  eta: 5:55:25    time: 1.2921  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2480/18659]  eta: 5:55:10    time: 1.2916  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2490/18659]  eta: 5:54:55    time: 1.2914  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2500/18659]  eta: 5:54:40    time: 1.2912  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2510/18659]  eta: 5:54:26    time: 1.2912  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2520/18659]  eta: 5:54:11    time: 1.2912  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2530/18659]  eta: 5:53:56    time: 1.2913  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2540/18659]  eta: 5:53:41    time: 1.2914  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2550/18659]  eta: 5:53:27    time: 1.2917  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2560/18659]  eta: 5:53:12    time: 1.2917  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2570/18659]  eta: 5:52:57    time: 1.2913  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2580/18659]  eta: 5:52:42    time: 1.2912  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2590/18659]  eta: 5:52:28    time: 1.2912  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2600/18659]  eta: 5:52:13    time: 1.2913  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2610/18659]  eta: 5:51:58    time: 1.2913  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2620/18659]  eta: 5:51:44    time: 1.2914  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2630/18659]  eta: 5:51:29    time: 1.2920  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2640/18659]  eta: 5:51:14    time: 1.2924  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2650/18659]  eta: 5:51:00    time: 1.2922  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2660/18659]  eta: 5:50:45    time: 1.2924  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2670/18659]  eta: 5:50:32    time: 1.3033  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2680/18659]  eta: 5:50:18    time: 1.3033  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2690/18659]  eta: 5:50:03    time: 1.2924  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2700/18659]  eta: 5:49:49    time: 1.2920  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2710/18659]  eta: 5:49:34    time: 1.2955  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2720/18659]  eta: 5:49:21    time: 1.3007  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2730/18659]  eta: 5:49:06    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2740/18659]  eta: 5:48:52    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2750/18659]  eta: 5:48:38    time: 1.3037  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2760/18659]  eta: 5:48:24    time: 1.3032  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2770/18659]  eta: 5:48:10    time: 1.2924  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2780/18659]  eta: 5:47:55    time: 1.2923  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2790/18659]  eta: 5:47:41    time: 1.2922  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2800/18659]  eta: 5:47:26    time: 1.2922  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2810/18659]  eta: 5:47:13    time: 1.3015  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2820/18659]  eta: 5:47:00    time: 1.3118  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2830/18659]  eta: 5:46:46    time: 1.3061  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2840/18659]  eta: 5:46:31    time: 1.2959  data: 0.0003  max mem: 7129\n",
      "Test:  [ 2850/18659]  eta: 5:46:17    time: 1.2925  data: 0.0003  max mem: 7129\n",
      "Test:  [ 2860/18659]  eta: 5:46:03    time: 1.2967  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2870/18659]  eta: 5:45:49    time: 1.2966  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2880/18659]  eta: 5:45:34    time: 1.2922  data: 0.0002  max mem: 7129\n",
      "Test:  [ 2890/18659]  eta: 5:45:20    time: 1.2924  data: 0.0003  max mem: 7129\n",
      "Test:  [ 2900/18659]  eta: 5:45:06    time: 1.2977  data: 0.0003  max mem: 7129\n",
      "Test:  [ 2910/18659]  eta: 5:44:52    time: 1.2984  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2920/18659]  eta: 5:44:38    time: 1.2935  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2930/18659]  eta: 5:44:24    time: 1.2928  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2940/18659]  eta: 5:44:09    time: 1.2927  data: 0.0001  max mem: 7129\n",
      "Test:  [ 2950/18659]  eta: 5:43:55    time: 1.2926  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2960/18659]  eta: 5:43:41    time: 1.2924  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2970/18659]  eta: 5:43:27    time: 1.2925  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2980/18659]  eta: 5:43:12    time: 1.2925  data: 0.0000  max mem: 7129\n",
      "Test:  [ 2990/18659]  eta: 5:42:58    time: 1.2924  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3000/18659]  eta: 5:42:44    time: 1.2928  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3010/18659]  eta: 5:42:30    time: 1.2934  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3020/18659]  eta: 5:42:16    time: 1.2935  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3030/18659]  eta: 5:42:01    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3040/18659]  eta: 5:41:47    time: 1.2928  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3050/18659]  eta: 5:41:33    time: 1.2928  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3060/18659]  eta: 5:41:19    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3070/18659]  eta: 5:41:05    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3080/18659]  eta: 5:40:51    time: 1.2927  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3090/18659]  eta: 5:40:36    time: 1.2927  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3100/18659]  eta: 5:40:22    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3110/18659]  eta: 5:40:09    time: 1.2969  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3120/18659]  eta: 5:39:55    time: 1.2967  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3130/18659]  eta: 5:39:40    time: 1.2929  data: 0.0003  max mem: 7129\n",
      "Test:  [ 3140/18659]  eta: 5:39:26    time: 1.2926  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3150/18659]  eta: 5:39:12    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3160/18659]  eta: 5:38:58    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3170/18659]  eta: 5:38:44    time: 1.2928  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3180/18659]  eta: 5:38:30    time: 1.2929  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3190/18659]  eta: 5:38:16    time: 1.2930  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3200/18659]  eta: 5:38:02    time: 1.2932  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3210/18659]  eta: 5:37:48    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3220/18659]  eta: 5:37:34    time: 1.2932  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3230/18659]  eta: 5:37:20    time: 1.2932  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3240/18659]  eta: 5:37:06    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3250/18659]  eta: 5:36:52    time: 1.2931  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3260/18659]  eta: 5:36:38    time: 1.2932  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3270/18659]  eta: 5:36:25    time: 1.3013  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3280/18659]  eta: 5:36:11    time: 1.3018  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3290/18659]  eta: 5:35:58    time: 1.3034  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3300/18659]  eta: 5:35:44    time: 1.3028  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3310/18659]  eta: 5:35:30    time: 1.2931  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3320/18659]  eta: 5:35:16    time: 1.2931  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3330/18659]  eta: 5:35:02    time: 1.2928  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3340/18659]  eta: 5:34:48    time: 1.2926  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3350/18659]  eta: 5:34:34    time: 1.2927  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3360/18659]  eta: 5:34:20    time: 1.2927  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3370/18659]  eta: 5:34:06    time: 1.2926  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3380/18659]  eta: 5:33:52    time: 1.2927  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3390/18659]  eta: 5:33:38    time: 1.2928  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3400/18659]  eta: 5:33:24    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3410/18659]  eta: 5:33:11    time: 1.3030  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3420/18659]  eta: 5:32:57    time: 1.3031  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3430/18659]  eta: 5:32:43    time: 1.2929  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3440/18659]  eta: 5:32:29    time: 1.2929  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3450/18659]  eta: 5:32:15    time: 1.2931  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3460/18659]  eta: 5:32:02    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3470/18659]  eta: 5:31:48    time: 1.2931  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3480/18659]  eta: 5:31:34    time: 1.2934  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3490/18659]  eta: 5:31:20    time: 1.2932  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3500/18659]  eta: 5:31:06    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3510/18659]  eta: 5:30:52    time: 1.2930  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3520/18659]  eta: 5:30:38    time: 1.2931  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3530/18659]  eta: 5:30:25    time: 1.2933  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3540/18659]  eta: 5:30:11    time: 1.2965  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3550/18659]  eta: 5:29:57    time: 1.2961  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3560/18659]  eta: 5:29:44    time: 1.3048  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3570/18659]  eta: 5:29:31    time: 1.3073  data: 0.0003  max mem: 7129\n",
      "Test:  [ 3580/18659]  eta: 5:29:17    time: 1.2955  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3590/18659]  eta: 5:29:03    time: 1.2930  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3600/18659]  eta: 5:28:49    time: 1.2930  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3610/18659]  eta: 5:28:35    time: 1.2931  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3620/18659]  eta: 5:28:22    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3630/18659]  eta: 5:28:09    time: 1.3046  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3640/18659]  eta: 5:27:55    time: 1.3044  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3650/18659]  eta: 5:27:41    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3660/18659]  eta: 5:27:27    time: 1.2943  data: 0.0003  max mem: 7129\n",
      "Test:  [ 3670/18659]  eta: 5:27:14    time: 1.2942  data: 0.0003  max mem: 7129\n",
      "Test:  [ 3680/18659]  eta: 5:27:00    time: 1.2969  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3690/18659]  eta: 5:26:46    time: 1.2964  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3700/18659]  eta: 5:26:33    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3710/18659]  eta: 5:26:19    time: 1.2935  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3720/18659]  eta: 5:26:05    time: 1.2936  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3730/18659]  eta: 5:25:51    time: 1.2935  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3740/18659]  eta: 5:25:38    time: 1.2936  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3750/18659]  eta: 5:25:24    time: 1.2937  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3760/18659]  eta: 5:25:10    time: 1.2936  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3770/18659]  eta: 5:24:56    time: 1.2935  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3780/18659]  eta: 5:24:43    time: 1.2933  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3790/18659]  eta: 5:24:29    time: 1.2931  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3800/18659]  eta: 5:24:15    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3810/18659]  eta: 5:24:02    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3820/18659]  eta: 5:23:48    time: 1.2934  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3830/18659]  eta: 5:23:34    time: 1.2935  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3840/18659]  eta: 5:23:20    time: 1.2938  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3850/18659]  eta: 5:23:07    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3860/18659]  eta: 5:22:53    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3870/18659]  eta: 5:22:39    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3880/18659]  eta: 5:22:26    time: 1.2932  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3890/18659]  eta: 5:22:12    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3900/18659]  eta: 5:21:58    time: 1.2928  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3910/18659]  eta: 5:21:45    time: 1.2930  data: 0.0001  max mem: 7129\n",
      "Test:  [ 3920/18659]  eta: 5:21:31    time: 1.2932  data: 0.0000  max mem: 7129\n",
      "Test:  [ 3930/18659]  eta: 5:21:17    time: 1.2937  data: 0.0003  max mem: 7129\n",
      "Test:  [ 3940/18659]  eta: 5:21:04    time: 1.2946  data: 0.0004  max mem: 7129\n",
      "Test:  [ 3950/18659]  eta: 5:20:50    time: 1.2950  data: 0.0003  max mem: 7129\n",
      "Test:  [ 3960/18659]  eta: 5:20:37    time: 1.2984  data: 0.0003  max mem: 7129\n",
      "Test:  [ 3970/18659]  eta: 5:20:23    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3980/18659]  eta: 5:20:09    time: 1.2933  data: 0.0002  max mem: 7129\n",
      "Test:  [ 3990/18659]  eta: 5:19:56    time: 1.2931  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4000/18659]  eta: 5:19:42    time: 1.2929  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4010/18659]  eta: 5:19:28    time: 1.2931  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4020/18659]  eta: 5:19:15    time: 1.2931  data: 0.0003  max mem: 7129\n",
      "Test:  [ 4030/18659]  eta: 5:19:01    time: 1.2932  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4040/18659]  eta: 5:18:47    time: 1.2934  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4050/18659]  eta: 5:18:34    time: 1.2935  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4060/18659]  eta: 5:18:20    time: 1.2935  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4070/18659]  eta: 5:18:07    time: 1.2966  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4080/18659]  eta: 5:17:53    time: 1.2965  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4090/18659]  eta: 5:17:40    time: 1.2931  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4100/18659]  eta: 5:17:26    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4110/18659]  eta: 5:17:13    time: 1.2976  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4120/18659]  eta: 5:16:59    time: 1.2979  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4130/18659]  eta: 5:16:46    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4140/18659]  eta: 5:16:32    time: 1.2937  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4150/18659]  eta: 5:16:18    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4160/18659]  eta: 5:16:05    time: 1.2934  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4170/18659]  eta: 5:15:51    time: 1.2934  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4180/18659]  eta: 5:15:38    time: 1.3040  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4190/18659]  eta: 5:15:25    time: 1.3040  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4200/18659]  eta: 5:15:11    time: 1.2936  data: 0.0003  max mem: 7129\n",
      "Test:  [ 4210/18659]  eta: 5:14:58    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4220/18659]  eta: 5:14:44    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4230/18659]  eta: 5:14:31    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4240/18659]  eta: 5:14:17    time: 1.2936  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4250/18659]  eta: 5:14:03    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4260/18659]  eta: 5:13:50    time: 1.2928  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4270/18659]  eta: 5:13:36    time: 1.2927  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4280/18659]  eta: 5:13:23    time: 1.2929  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4290/18659]  eta: 5:13:09    time: 1.2931  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4300/18659]  eta: 5:12:56    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4310/18659]  eta: 5:12:42    time: 1.2933  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4320/18659]  eta: 5:12:28    time: 1.2934  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4330/18659]  eta: 5:12:17    time: 1.3224  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4340/18659]  eta: 5:12:03    time: 1.3240  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4350/18659]  eta: 5:11:50    time: 1.2964  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4360/18659]  eta: 5:11:36    time: 1.2962  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4370/18659]  eta: 5:11:23    time: 1.2963  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4380/18659]  eta: 5:11:10    time: 1.2966  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4390/18659]  eta: 5:10:56    time: 1.2969  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4400/18659]  eta: 5:10:43    time: 1.2972  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4410/18659]  eta: 5:10:29    time: 1.2974  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4420/18659]  eta: 5:10:16    time: 1.2971  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4430/18659]  eta: 5:10:03    time: 1.2966  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4440/18659]  eta: 5:09:49    time: 1.2966  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4450/18659]  eta: 5:09:36    time: 1.2965  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4460/18659]  eta: 5:09:22    time: 1.2968  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4470/18659]  eta: 5:09:09    time: 1.2971  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4480/18659]  eta: 5:08:56    time: 1.2973  data: 0.0003  max mem: 7129\n",
      "Test:  [ 4490/18659]  eta: 5:08:42    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4500/18659]  eta: 5:08:29    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4510/18659]  eta: 5:08:15    time: 1.2973  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4520/18659]  eta: 5:08:02    time: 1.2972  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4530/18659]  eta: 5:07:49    time: 1.2972  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4540/18659]  eta: 5:07:36    time: 1.3106  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4550/18659]  eta: 5:07:23    time: 1.3110  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4560/18659]  eta: 5:07:09    time: 1.3005  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4570/18659]  eta: 5:06:56    time: 1.3006  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4580/18659]  eta: 5:06:43    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4590/18659]  eta: 5:06:29    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4600/18659]  eta: 5:06:16    time: 1.2980  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4610/18659]  eta: 5:06:03    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4620/18659]  eta: 5:05:49    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4630/18659]  eta: 5:05:36    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4640/18659]  eta: 5:05:23    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4650/18659]  eta: 5:05:09    time: 1.2978  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4660/18659]  eta: 5:04:56    time: 1.2979  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4670/18659]  eta: 5:04:43    time: 1.2981  data: 0.0003  max mem: 7129\n",
      "Test:  [ 4680/18659]  eta: 5:04:29    time: 1.2987  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4690/18659]  eta: 5:04:16    time: 1.2988  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4700/18659]  eta: 5:04:03    time: 1.2984  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4710/18659]  eta: 5:03:49    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4720/18659]  eta: 5:03:36    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4730/18659]  eta: 5:03:23    time: 1.2981  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4740/18659]  eta: 5:03:09    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4750/18659]  eta: 5:02:56    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4760/18659]  eta: 5:02:43    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4770/18659]  eta: 5:02:29    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4780/18659]  eta: 5:02:17    time: 1.3138  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4790/18659]  eta: 5:02:04    time: 1.3152  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4800/18659]  eta: 5:01:50    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4810/18659]  eta: 5:01:37    time: 1.2949  data: 0.0003  max mem: 7129\n",
      "Test:  [ 4820/18659]  eta: 5:01:25    time: 1.3117  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4830/18659]  eta: 5:01:12    time: 1.3225  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4840/18659]  eta: 5:00:58    time: 1.3063  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4850/18659]  eta: 5:00:45    time: 1.2954  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4860/18659]  eta: 5:00:32    time: 1.2948  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4870/18659]  eta: 5:00:18    time: 1.2949  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4880/18659]  eta: 5:00:05    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4890/18659]  eta: 4:59:51    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4900/18659]  eta: 4:59:38    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4910/18659]  eta: 4:59:25    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4920/18659]  eta: 4:59:11    time: 1.2947  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4930/18659]  eta: 4:58:58    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [ 4940/18659]  eta: 4:58:44    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4950/18659]  eta: 4:58:31    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4960/18659]  eta: 4:58:18    time: 1.2954  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4970/18659]  eta: 4:58:04    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [ 4980/18659]  eta: 4:57:51    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [ 4990/18659]  eta: 4:57:38    time: 1.3053  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5000/18659]  eta: 4:57:25    time: 1.3085  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5010/18659]  eta: 4:57:11    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5020/18659]  eta: 4:56:58    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5030/18659]  eta: 4:56:45    time: 1.2943  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5040/18659]  eta: 4:56:31    time: 1.2946  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5050/18659]  eta: 4:56:18    time: 1.2952  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5060/18659]  eta: 4:56:04    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5070/18659]  eta: 4:55:51    time: 1.2948  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5080/18659]  eta: 4:55:38    time: 1.2946  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5090/18659]  eta: 4:55:24    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5100/18659]  eta: 4:55:11    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5110/18659]  eta: 4:54:58    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5120/18659]  eta: 4:54:44    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5130/18659]  eta: 4:54:31    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5140/18659]  eta: 4:54:18    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5150/18659]  eta: 4:54:05    time: 1.3120  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5160/18659]  eta: 4:53:52    time: 1.3119  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5170/18659]  eta: 4:53:38    time: 1.2947  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5180/18659]  eta: 4:53:25    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5190/18659]  eta: 4:53:12    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5200/18659]  eta: 4:52:58    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5210/18659]  eta: 4:52:45    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5220/18659]  eta: 4:52:32    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5230/18659]  eta: 4:52:18    time: 1.2955  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5240/18659]  eta: 4:52:05    time: 1.2956  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5250/18659]  eta: 4:51:52    time: 1.2951  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5260/18659]  eta: 4:51:39    time: 1.3124  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5270/18659]  eta: 4:51:26    time: 1.3183  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5280/18659]  eta: 4:51:13    time: 1.3020  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5290/18659]  eta: 4:50:59    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5300/18659]  eta: 4:50:46    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5310/18659]  eta: 4:50:33    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5320/18659]  eta: 4:50:20    time: 1.2981  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5330/18659]  eta: 4:50:06    time: 1.2988  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5340/18659]  eta: 4:49:53    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5350/18659]  eta: 4:49:40    time: 1.2979  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5360/18659]  eta: 4:49:27    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5370/18659]  eta: 4:49:13    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5380/18659]  eta: 4:49:00    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5390/18659]  eta: 4:48:47    time: 1.2980  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5400/18659]  eta: 4:48:34    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5410/18659]  eta: 4:48:20    time: 1.2981  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5420/18659]  eta: 4:48:07    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5430/18659]  eta: 4:47:54    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5440/18659]  eta: 4:47:41    time: 1.2981  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5450/18659]  eta: 4:47:27    time: 1.2979  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5460/18659]  eta: 4:47:14    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5470/18659]  eta: 4:47:01    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5480/18659]  eta: 4:46:48    time: 1.3002  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5490/18659]  eta: 4:46:35    time: 1.3142  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5500/18659]  eta: 4:46:23    time: 1.3244  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5510/18659]  eta: 4:46:09    time: 1.3093  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5520/18659]  eta: 4:45:56    time: 1.2956  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5530/18659]  eta: 4:45:43    time: 1.3091  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5540/18659]  eta: 4:45:30    time: 1.3087  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5550/18659]  eta: 4:45:18    time: 1.3226  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5560/18659]  eta: 4:45:05    time: 1.3259  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5570/18659]  eta: 4:44:51    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5580/18659]  eta: 4:44:38    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5590/18659]  eta: 4:44:25    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5600/18659]  eta: 4:44:12    time: 1.2953  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5610/18659]  eta: 4:43:58    time: 1.2955  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5620/18659]  eta: 4:43:45    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5630/18659]  eta: 4:43:32    time: 1.2947  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5640/18659]  eta: 4:43:19    time: 1.3168  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5650/18659]  eta: 4:43:07    time: 1.3283  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5660/18659]  eta: 4:42:53    time: 1.3060  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5670/18659]  eta: 4:42:40    time: 1.2947  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5680/18659]  eta: 4:42:27    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5690/18659]  eta: 4:42:13    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5700/18659]  eta: 4:42:00    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5710/18659]  eta: 4:41:47    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5720/18659]  eta: 4:41:33    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5730/18659]  eta: 4:41:20    time: 1.2946  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5740/18659]  eta: 4:41:07    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5750/18659]  eta: 4:40:54    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5760/18659]  eta: 4:40:40    time: 1.2945  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5770/18659]  eta: 4:40:27    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5780/18659]  eta: 4:40:14    time: 1.2950  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5790/18659]  eta: 4:40:00    time: 1.2955  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5800/18659]  eta: 4:39:47    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5810/18659]  eta: 4:39:34    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5820/18659]  eta: 4:39:20    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5830/18659]  eta: 4:39:07    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5840/18659]  eta: 4:38:54    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5850/18659]  eta: 4:38:41    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5860/18659]  eta: 4:38:27    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5870/18659]  eta: 4:38:14    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5880/18659]  eta: 4:38:01    time: 1.2952  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5890/18659]  eta: 4:37:47    time: 1.2951  data: 0.0000  max mem: 7129\n",
      "Test:  [ 5900/18659]  eta: 4:37:34    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5910/18659]  eta: 4:37:21    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5920/18659]  eta: 4:37:08    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5930/18659]  eta: 4:36:54    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5940/18659]  eta: 4:36:41    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5950/18659]  eta: 4:36:28    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5960/18659]  eta: 4:36:14    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 5970/18659]  eta: 4:36:01    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [ 5980/18659]  eta: 4:35:48    time: 1.2943  data: 0.0003  max mem: 7129\n",
      "Test:  [ 5990/18659]  eta: 4:35:35    time: 1.2990  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6000/18659]  eta: 4:35:21    time: 1.2993  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6010/18659]  eta: 4:35:08    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6020/18659]  eta: 4:34:55    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6030/18659]  eta: 4:34:42    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6040/18659]  eta: 4:34:28    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6050/18659]  eta: 4:34:15    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6060/18659]  eta: 4:34:02    time: 1.3015  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6070/18659]  eta: 4:33:49    time: 1.2987  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6080/18659]  eta: 4:33:36    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6090/18659]  eta: 4:33:22    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6100/18659]  eta: 4:33:09    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6110/18659]  eta: 4:32:56    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6120/18659]  eta: 4:32:43    time: 1.3025  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6130/18659]  eta: 4:32:30    time: 1.3025  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6140/18659]  eta: 4:32:16    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6150/18659]  eta: 4:32:03    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6160/18659]  eta: 4:31:50    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6170/18659]  eta: 4:31:37    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6180/18659]  eta: 4:31:23    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6190/18659]  eta: 4:31:10    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6200/18659]  eta: 4:30:57    time: 1.2939  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6210/18659]  eta: 4:30:44    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6220/18659]  eta: 4:30:30    time: 1.2944  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6230/18659]  eta: 4:30:17    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6240/18659]  eta: 4:30:04    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6250/18659]  eta: 4:29:50    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6260/18659]  eta: 4:29:37    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6270/18659]  eta: 4:29:24    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6280/18659]  eta: 4:29:11    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6290/18659]  eta: 4:28:57    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6300/18659]  eta: 4:28:44    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6310/18659]  eta: 4:28:31    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6320/18659]  eta: 4:28:18    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6330/18659]  eta: 4:28:05    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6340/18659]  eta: 4:27:51    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6350/18659]  eta: 4:27:38    time: 1.2954  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6360/18659]  eta: 4:27:25    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6370/18659]  eta: 4:27:12    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6380/18659]  eta: 4:26:58    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6390/18659]  eta: 4:26:45    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6400/18659]  eta: 4:26:32    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6410/18659]  eta: 4:26:19    time: 1.2942  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6420/18659]  eta: 4:26:05    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6430/18659]  eta: 4:25:52    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6440/18659]  eta: 4:25:39    time: 1.2953  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6450/18659]  eta: 4:25:26    time: 1.2957  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6460/18659]  eta: 4:25:13    time: 1.2954  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6470/18659]  eta: 4:24:59    time: 1.2944  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6480/18659]  eta: 4:24:46    time: 1.2995  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6490/18659]  eta: 4:24:33    time: 1.2997  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6500/18659]  eta: 4:24:21    time: 1.3241  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6510/18659]  eta: 4:24:08    time: 1.3304  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6520/18659]  eta: 4:23:55    time: 1.3025  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6530/18659]  eta: 4:23:42    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6540/18659]  eta: 4:23:28    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6550/18659]  eta: 4:23:15    time: 1.3009  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6560/18659]  eta: 4:23:02    time: 1.3009  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6570/18659]  eta: 4:22:49    time: 1.2973  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6580/18659]  eta: 4:22:36    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6590/18659]  eta: 4:22:23    time: 1.2975  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6600/18659]  eta: 4:22:10    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6610/18659]  eta: 4:21:56    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6620/18659]  eta: 4:21:43    time: 1.2986  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6630/18659]  eta: 4:21:30    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6640/18659]  eta: 4:21:17    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6650/18659]  eta: 4:21:04    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6660/18659]  eta: 4:20:51    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6670/18659]  eta: 4:20:37    time: 1.2976  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6680/18659]  eta: 4:20:24    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6690/18659]  eta: 4:20:11    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6700/18659]  eta: 4:19:58    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6710/18659]  eta: 4:19:45    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6720/18659]  eta: 4:19:32    time: 1.2983  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6730/18659]  eta: 4:19:19    time: 1.2980  data: 0.0004  max mem: 7129\n",
      "Test:  [ 6740/18659]  eta: 4:19:05    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6750/18659]  eta: 4:18:52    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6760/18659]  eta: 4:18:39    time: 1.3030  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6770/18659]  eta: 4:18:26    time: 1.3059  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6780/18659]  eta: 4:18:13    time: 1.3091  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6790/18659]  eta: 4:18:00    time: 1.3096  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6800/18659]  eta: 4:17:47    time: 1.3013  data: 0.0000  max mem: 7129\n",
      "Test:  [ 6810/18659]  eta: 4:17:34    time: 1.3013  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6820/18659]  eta: 4:17:21    time: 1.3013  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6830/18659]  eta: 4:17:08    time: 1.2979  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6840/18659]  eta: 4:16:55    time: 1.3181  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6850/18659]  eta: 4:16:42    time: 1.3208  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6860/18659]  eta: 4:16:29    time: 1.2992  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6870/18659]  eta: 4:16:16    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6880/18659]  eta: 4:16:03    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6890/18659]  eta: 4:15:49    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6900/18659]  eta: 4:15:36    time: 1.2990  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6910/18659]  eta: 4:15:23    time: 1.2988  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6920/18659]  eta: 4:15:10    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6930/18659]  eta: 4:14:57    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6940/18659]  eta: 4:14:44    time: 1.2946  data: 0.0003  max mem: 7129\n",
      "Test:  [ 6950/18659]  eta: 4:14:30    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6960/18659]  eta: 4:14:17    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 6970/18659]  eta: 4:14:04    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6980/18659]  eta: 4:13:51    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [ 6990/18659]  eta: 4:13:38    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7000/18659]  eta: 4:13:24    time: 1.2953  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7010/18659]  eta: 4:13:11    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7020/18659]  eta: 4:12:58    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7030/18659]  eta: 4:12:45    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7040/18659]  eta: 4:12:32    time: 1.2941  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7050/18659]  eta: 4:12:18    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7060/18659]  eta: 4:12:05    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7070/18659]  eta: 4:11:52    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7080/18659]  eta: 4:11:39    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7090/18659]  eta: 4:11:26    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7100/18659]  eta: 4:11:12    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7110/18659]  eta: 4:11:00    time: 1.3224  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7120/18659]  eta: 4:10:47    time: 1.3238  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7130/18659]  eta: 4:10:34    time: 1.3081  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7140/18659]  eta: 4:10:21    time: 1.3082  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7150/18659]  eta: 4:10:08    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7160/18659]  eta: 4:09:55    time: 1.2977  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7170/18659]  eta: 4:09:42    time: 1.3003  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7180/18659]  eta: 4:09:29    time: 1.3126  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7190/18659]  eta: 4:09:16    time: 1.3107  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7200/18659]  eta: 4:09:03    time: 1.2996  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7210/18659]  eta: 4:08:50    time: 1.2991  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7220/18659]  eta: 4:08:36    time: 1.2979  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7230/18659]  eta: 4:08:23    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7240/18659]  eta: 4:08:10    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7250/18659]  eta: 4:07:57    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7260/18659]  eta: 4:07:44    time: 1.3008  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7270/18659]  eta: 4:07:31    time: 1.3019  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7280/18659]  eta: 4:07:18    time: 1.3015  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7290/18659]  eta: 4:07:05    time: 1.3010  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7300/18659]  eta: 4:06:52    time: 1.3006  data: 0.0003  max mem: 7129\n",
      "Test:  [ 7310/18659]  eta: 4:06:38    time: 1.3004  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7320/18659]  eta: 4:06:25    time: 1.3003  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7330/18659]  eta: 4:06:12    time: 1.3034  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7340/18659]  eta: 4:05:59    time: 1.3031  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7350/18659]  eta: 4:05:46    time: 1.3003  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7360/18659]  eta: 4:05:33    time: 1.2987  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7370/18659]  eta: 4:05:20    time: 1.2981  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7380/18659]  eta: 4:05:07    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7390/18659]  eta: 4:04:54    time: 1.2981  data: 0.0003  max mem: 7129\n",
      "Test:  [ 7400/18659]  eta: 4:04:41    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7410/18659]  eta: 4:04:27    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7420/18659]  eta: 4:04:14    time: 1.2978  data: 0.0003  max mem: 7129\n",
      "Test:  [ 7430/18659]  eta: 4:04:01    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7440/18659]  eta: 4:03:48    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7450/18659]  eta: 4:03:35    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7460/18659]  eta: 4:03:22    time: 1.2987  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7470/18659]  eta: 4:03:09    time: 1.2981  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7480/18659]  eta: 4:02:55    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7490/18659]  eta: 4:02:42    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7500/18659]  eta: 4:02:29    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7510/18659]  eta: 4:02:16    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7520/18659]  eta: 4:02:03    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7530/18659]  eta: 4:01:50    time: 1.2979  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7540/18659]  eta: 4:01:37    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7550/18659]  eta: 4:01:24    time: 1.2984  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7560/18659]  eta: 4:01:10    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7570/18659]  eta: 4:00:57    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7580/18659]  eta: 4:00:44    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7590/18659]  eta: 4:00:31    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7600/18659]  eta: 4:00:18    time: 1.2978  data: 0.0003  max mem: 7129\n",
      "Test:  [ 7610/18659]  eta: 4:00:05    time: 1.2978  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7620/18659]  eta: 3:59:52    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7630/18659]  eta: 3:59:39    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7640/18659]  eta: 3:59:25    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7650/18659]  eta: 3:59:12    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7660/18659]  eta: 3:58:59    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7670/18659]  eta: 3:58:46    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7680/18659]  eta: 3:58:33    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7690/18659]  eta: 3:58:20    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7700/18659]  eta: 3:58:07    time: 1.3019  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7710/18659]  eta: 3:57:54    time: 1.3019  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7720/18659]  eta: 3:57:41    time: 1.3155  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7730/18659]  eta: 3:57:28    time: 1.3157  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7740/18659]  eta: 3:57:15    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7750/18659]  eta: 3:57:02    time: 1.3175  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7760/18659]  eta: 3:56:49    time: 1.3165  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7770/18659]  eta: 3:56:36    time: 1.2957  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7780/18659]  eta: 3:56:23    time: 1.2980  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7790/18659]  eta: 3:56:10    time: 1.2979  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7800/18659]  eta: 3:55:57    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7810/18659]  eta: 3:55:43    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7820/18659]  eta: 3:55:30    time: 1.2952  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7830/18659]  eta: 3:55:17    time: 1.2952  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7840/18659]  eta: 3:55:04    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7850/18659]  eta: 3:54:51    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7860/18659]  eta: 3:54:38    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7870/18659]  eta: 3:54:24    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7880/18659]  eta: 3:54:11    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7890/18659]  eta: 3:53:58    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7900/18659]  eta: 3:53:45    time: 1.2944  data: 0.0003  max mem: 7129\n",
      "Test:  [ 7910/18659]  eta: 3:53:32    time: 1.3027  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7920/18659]  eta: 3:53:19    time: 1.3027  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7930/18659]  eta: 3:53:06    time: 1.3083  data: 0.0002  max mem: 7129\n",
      "Test:  [ 7940/18659]  eta: 3:52:53    time: 1.3084  data: 0.0000  max mem: 7129\n",
      "Test:  [ 7950/18659]  eta: 3:52:40    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7960/18659]  eta: 3:52:27    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7970/18659]  eta: 3:52:14    time: 1.3015  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7980/18659]  eta: 3:52:01    time: 1.3018  data: 0.0001  max mem: 7129\n",
      "Test:  [ 7990/18659]  eta: 3:51:47    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8000/18659]  eta: 3:51:34    time: 1.2944  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8010/18659]  eta: 3:51:21    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8020/18659]  eta: 3:51:08    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8030/18659]  eta: 3:50:55    time: 1.2974  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8040/18659]  eta: 3:50:42    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8050/18659]  eta: 3:50:28    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8060/18659]  eta: 3:50:15    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8070/18659]  eta: 3:50:02    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8080/18659]  eta: 3:49:49    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8090/18659]  eta: 3:49:36    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8100/18659]  eta: 3:49:23    time: 1.2949  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8110/18659]  eta: 3:49:10    time: 1.2944  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8120/18659]  eta: 3:48:56    time: 1.2937  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8130/18659]  eta: 3:48:43    time: 1.2938  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8140/18659]  eta: 3:48:30    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8150/18659]  eta: 3:48:17    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8160/18659]  eta: 3:48:04    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8170/18659]  eta: 3:47:51    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8180/18659]  eta: 3:47:37    time: 1.2939  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8190/18659]  eta: 3:47:24    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8200/18659]  eta: 3:47:11    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8210/18659]  eta: 3:46:58    time: 1.2941  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8220/18659]  eta: 3:46:45    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8230/18659]  eta: 3:46:32    time: 1.2938  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8240/18659]  eta: 3:46:19    time: 1.2937  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8250/18659]  eta: 3:46:05    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8260/18659]  eta: 3:45:52    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8270/18659]  eta: 3:45:39    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8280/18659]  eta: 3:45:26    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8290/18659]  eta: 3:45:13    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8300/18659]  eta: 3:45:00    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8310/18659]  eta: 3:44:47    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8320/18659]  eta: 3:44:33    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8330/18659]  eta: 3:44:20    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8340/18659]  eta: 3:44:07    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8350/18659]  eta: 3:43:54    time: 1.2938  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8360/18659]  eta: 3:43:41    time: 1.2938  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8370/18659]  eta: 3:43:28    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8380/18659]  eta: 3:43:15    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8390/18659]  eta: 3:43:01    time: 1.2939  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8400/18659]  eta: 3:42:48    time: 1.2938  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8410/18659]  eta: 3:42:35    time: 1.2937  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8420/18659]  eta: 3:42:22    time: 1.2979  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8430/18659]  eta: 3:42:09    time: 1.2979  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8440/18659]  eta: 3:41:56    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8450/18659]  eta: 3:41:43    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8460/18659]  eta: 3:41:30    time: 1.2974  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8470/18659]  eta: 3:41:16    time: 1.2975  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8480/18659]  eta: 3:41:03    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8490/18659]  eta: 3:40:50    time: 1.2938  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8500/18659]  eta: 3:40:37    time: 1.2936  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8510/18659]  eta: 3:40:24    time: 1.2935  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8520/18659]  eta: 3:40:11    time: 1.2937  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8530/18659]  eta: 3:39:58    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8540/18659]  eta: 3:39:44    time: 1.2938  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8550/18659]  eta: 3:39:31    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8560/18659]  eta: 3:39:18    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8570/18659]  eta: 3:39:05    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8580/18659]  eta: 3:38:52    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8590/18659]  eta: 3:38:39    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8600/18659]  eta: 3:38:26    time: 1.2940  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8610/18659]  eta: 3:38:12    time: 1.2939  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8620/18659]  eta: 3:37:59    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8630/18659]  eta: 3:37:46    time: 1.2937  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8640/18659]  eta: 3:37:33    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8650/18659]  eta: 3:37:20    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8660/18659]  eta: 3:37:07    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8670/18659]  eta: 3:36:54    time: 1.3050  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8680/18659]  eta: 3:36:41    time: 1.3046  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8690/18659]  eta: 3:36:28    time: 1.2936  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8700/18659]  eta: 3:36:15    time: 1.2936  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8710/18659]  eta: 3:36:01    time: 1.2940  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8720/18659]  eta: 3:35:48    time: 1.2939  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8730/18659]  eta: 3:35:35    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8740/18659]  eta: 3:35:22    time: 1.2937  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8750/18659]  eta: 3:35:09    time: 1.3187  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8760/18659]  eta: 3:34:56    time: 1.3231  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8770/18659]  eta: 3:34:43    time: 1.2997  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8780/18659]  eta: 3:34:30    time: 1.2968  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8790/18659]  eta: 3:34:17    time: 1.2970  data: 0.0003  max mem: 7129\n",
      "Test:  [ 8800/18659]  eta: 3:34:04    time: 1.2971  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8810/18659]  eta: 3:33:51    time: 1.2973  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8820/18659]  eta: 3:33:38    time: 1.2975  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8830/18659]  eta: 3:33:25    time: 1.2978  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8840/18659]  eta: 3:33:12    time: 1.3072  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8850/18659]  eta: 3:32:59    time: 1.3071  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8860/18659]  eta: 3:32:46    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8870/18659]  eta: 3:32:33    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8880/18659]  eta: 3:32:20    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8890/18659]  eta: 3:32:07    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8900/18659]  eta: 3:31:53    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8910/18659]  eta: 3:31:40    time: 1.2974  data: 0.0002  max mem: 7129\n",
      "Test:  [ 8920/18659]  eta: 3:31:27    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8930/18659]  eta: 3:31:14    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8940/18659]  eta: 3:31:01    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8950/18659]  eta: 3:30:48    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8960/18659]  eta: 3:30:35    time: 1.2981  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8970/18659]  eta: 3:30:22    time: 1.2980  data: 0.0000  max mem: 7129\n",
      "Test:  [ 8980/18659]  eta: 3:30:09    time: 1.3046  data: 0.0001  max mem: 7129\n",
      "Test:  [ 8990/18659]  eta: 3:29:56    time: 1.3073  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9000/18659]  eta: 3:29:43    time: 1.2992  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9010/18659]  eta: 3:29:30    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9020/18659]  eta: 3:29:17    time: 1.3098  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9030/18659]  eta: 3:29:04    time: 1.3098  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9040/18659]  eta: 3:28:51    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9050/18659]  eta: 3:28:38    time: 1.2947  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9060/18659]  eta: 3:28:24    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9070/18659]  eta: 3:28:11    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9080/18659]  eta: 3:27:58    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9090/18659]  eta: 3:27:45    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9100/18659]  eta: 3:27:32    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9110/18659]  eta: 3:27:19    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9120/18659]  eta: 3:27:06    time: 1.2953  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9130/18659]  eta: 3:26:53    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9140/18659]  eta: 3:26:40    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9150/18659]  eta: 3:26:26    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9160/18659]  eta: 3:26:13    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9170/18659]  eta: 3:26:00    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9180/18659]  eta: 3:25:47    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9190/18659]  eta: 3:25:34    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [ 9200/18659]  eta: 3:25:21    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9210/18659]  eta: 3:25:08    time: 1.2952  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9220/18659]  eta: 3:24:55    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9230/18659]  eta: 3:24:42    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9240/18659]  eta: 3:24:28    time: 1.2945  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9250/18659]  eta: 3:24:15    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9260/18659]  eta: 3:24:02    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9270/18659]  eta: 3:23:49    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9280/18659]  eta: 3:23:36    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9290/18659]  eta: 3:23:23    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9300/18659]  eta: 3:23:10    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9310/18659]  eta: 3:22:57    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9320/18659]  eta: 3:22:44    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9330/18659]  eta: 3:22:31    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9340/18659]  eta: 3:22:18    time: 1.2948  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9350/18659]  eta: 3:22:04    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9360/18659]  eta: 3:21:51    time: 1.2947  data: 0.0000  max mem: 7129\n",
      "Test:  [ 9370/18659]  eta: 3:21:38    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9380/18659]  eta: 3:21:25    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9390/18659]  eta: 3:21:12    time: 1.2956  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9400/18659]  eta: 3:20:59    time: 1.2956  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9410/18659]  eta: 3:20:46    time: 1.2990  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9420/18659]  eta: 3:20:33    time: 1.2988  data: 0.0000  max mem: 7129\n",
      "Test:  [ 9430/18659]  eta: 3:20:20    time: 1.2972  data: 0.0000  max mem: 7129\n",
      "Test:  [ 9440/18659]  eta: 3:20:07    time: 1.2972  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9450/18659]  eta: 3:19:54    time: 1.2949  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9460/18659]  eta: 3:19:40    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9470/18659]  eta: 3:19:27    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9480/18659]  eta: 3:19:14    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9490/18659]  eta: 3:19:01    time: 1.2953  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9500/18659]  eta: 3:18:48    time: 1.2949  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9510/18659]  eta: 3:18:35    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9520/18659]  eta: 3:18:22    time: 1.3006  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9530/18659]  eta: 3:18:09    time: 1.3005  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9540/18659]  eta: 3:17:56    time: 1.2947  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9550/18659]  eta: 3:17:43    time: 1.2945  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9560/18659]  eta: 3:17:30    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9570/18659]  eta: 3:17:16    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9580/18659]  eta: 3:17:03    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9590/18659]  eta: 3:16:50    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9600/18659]  eta: 3:16:37    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9610/18659]  eta: 3:16:24    time: 1.2944  data: 0.0003  max mem: 7129\n",
      "Test:  [ 9620/18659]  eta: 3:16:11    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9630/18659]  eta: 3:15:58    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9640/18659]  eta: 3:15:45    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9650/18659]  eta: 3:15:32    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9660/18659]  eta: 3:15:19    time: 1.3141  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9670/18659]  eta: 3:15:06    time: 1.3144  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9680/18659]  eta: 3:14:53    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [ 9690/18659]  eta: 3:14:40    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [ 9700/18659]  eta: 3:14:27    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9710/18659]  eta: 3:14:14    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9720/18659]  eta: 3:14:00    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9730/18659]  eta: 3:13:47    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9740/18659]  eta: 3:13:34    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9750/18659]  eta: 3:13:21    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9760/18659]  eta: 3:13:08    time: 1.2952  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9770/18659]  eta: 3:12:55    time: 1.3175  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9780/18659]  eta: 3:12:42    time: 1.3196  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9790/18659]  eta: 3:12:29    time: 1.3043  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9800/18659]  eta: 3:12:16    time: 1.3018  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9810/18659]  eta: 3:12:03    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9820/18659]  eta: 3:11:50    time: 1.3057  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9830/18659]  eta: 3:11:37    time: 1.3132  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9840/18659]  eta: 3:11:24    time: 1.3063  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9850/18659]  eta: 3:11:11    time: 1.2988  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9860/18659]  eta: 3:10:58    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9870/18659]  eta: 3:10:46    time: 1.3294  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9880/18659]  eta: 3:10:33    time: 1.3363  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9890/18659]  eta: 3:10:20    time: 1.3091  data: 0.0004  max mem: 7129\n",
      "Test:  [ 9900/18659]  eta: 3:10:07    time: 1.3039  data: 0.0004  max mem: 7129\n",
      "Test:  [ 9910/18659]  eta: 3:09:54    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9920/18659]  eta: 3:09:41    time: 1.2984  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9930/18659]  eta: 3:09:28    time: 1.2982  data: 0.0000  max mem: 7129\n",
      "Test:  [ 9940/18659]  eta: 3:09:15    time: 1.2986  data: 0.0000  max mem: 7129\n",
      "Test:  [ 9950/18659]  eta: 3:09:01    time: 1.2991  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9960/18659]  eta: 3:08:48    time: 1.2990  data: 0.0002  max mem: 7129\n",
      "Test:  [ 9970/18659]  eta: 3:08:35    time: 1.2988  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9980/18659]  eta: 3:08:22    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [ 9990/18659]  eta: 3:08:09    time: 1.2981  data: 0.0000  max mem: 7129\n",
      "Test:  [10000/18659]  eta: 3:07:56    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [10010/18659]  eta: 3:07:43    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10020/18659]  eta: 3:07:30    time: 1.2980  data: 0.0000  max mem: 7129\n",
      "Test:  [10030/18659]  eta: 3:07:17    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10040/18659]  eta: 3:07:04    time: 1.2984  data: 0.0002  max mem: 7129\n",
      "Test:  [10050/18659]  eta: 3:06:51    time: 1.2984  data: 0.0002  max mem: 7129\n",
      "Test:  [10060/18659]  eta: 3:06:38    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10070/18659]  eta: 3:06:25    time: 1.2980  data: 0.0000  max mem: 7129\n",
      "Test:  [10080/18659]  eta: 3:06:12    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10090/18659]  eta: 3:05:59    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10100/18659]  eta: 3:05:46    time: 1.3002  data: 0.0001  max mem: 7129\n",
      "Test:  [10110/18659]  eta: 3:05:33    time: 1.3007  data: 0.0001  max mem: 7129\n",
      "Test:  [10120/18659]  eta: 3:05:20    time: 1.2987  data: 0.0001  max mem: 7129\n",
      "Test:  [10130/18659]  eta: 3:05:06    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [10140/18659]  eta: 3:04:53    time: 1.2980  data: 0.0000  max mem: 7129\n",
      "Test:  [10150/18659]  eta: 3:04:40    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10160/18659]  eta: 3:04:27    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10170/18659]  eta: 3:04:14    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [10180/18659]  eta: 3:04:01    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [10190/18659]  eta: 3:03:48    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [10200/18659]  eta: 3:03:35    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [10210/18659]  eta: 3:03:22    time: 1.3211  data: 0.0001  max mem: 7129\n",
      "Test:  [10220/18659]  eta: 3:03:09    time: 1.3216  data: 0.0002  max mem: 7129\n",
      "Test:  [10230/18659]  eta: 3:02:56    time: 1.2988  data: 0.0003  max mem: 7129\n",
      "Test:  [10240/18659]  eta: 3:02:43    time: 1.3075  data: 0.0002  max mem: 7129\n",
      "Test:  [10250/18659]  eta: 3:02:30    time: 1.3070  data: 0.0000  max mem: 7129\n",
      "Test:  [10260/18659]  eta: 3:02:17    time: 1.2978  data: 0.0000  max mem: 7129\n",
      "Test:  [10270/18659]  eta: 3:02:04    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [10280/18659]  eta: 3:01:51    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [10290/18659]  eta: 3:01:38    time: 1.2978  data: 0.0003  max mem: 7129\n",
      "Test:  [10300/18659]  eta: 3:01:25    time: 1.2980  data: 0.0002  max mem: 7129\n",
      "Test:  [10310/18659]  eta: 3:01:12    time: 1.2983  data: 0.0002  max mem: 7129\n",
      "Test:  [10320/18659]  eta: 3:00:59    time: 1.2987  data: 0.0003  max mem: 7129\n",
      "Test:  [10330/18659]  eta: 3:00:46    time: 1.2987  data: 0.0002  max mem: 7129\n",
      "Test:  [10340/18659]  eta: 3:00:33    time: 1.3225  data: 0.0001  max mem: 7129\n",
      "Test:  [10350/18659]  eta: 3:00:20    time: 1.3228  data: 0.0001  max mem: 7129\n",
      "Test:  [10360/18659]  eta: 3:00:07    time: 1.2986  data: 0.0000  max mem: 7129\n",
      "Test:  [10370/18659]  eta: 2:59:54    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [10380/18659]  eta: 2:59:41    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [10390/18659]  eta: 2:59:28    time: 1.3041  data: 0.0001  max mem: 7129\n",
      "Test:  [10400/18659]  eta: 2:59:15    time: 1.3038  data: 0.0001  max mem: 7129\n",
      "Test:  [10410/18659]  eta: 2:59:02    time: 1.2981  data: 0.0002  max mem: 7129\n",
      "Test:  [10420/18659]  eta: 2:58:49    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [10430/18659]  eta: 2:58:36    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10440/18659]  eta: 2:58:23    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10450/18659]  eta: 2:58:10    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [10460/18659]  eta: 2:57:57    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [10470/18659]  eta: 2:57:44    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [10480/18659]  eta: 2:57:31    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [10490/18659]  eta: 2:57:17    time: 1.2982  data: 0.0000  max mem: 7129\n",
      "Test:  [10500/18659]  eta: 2:57:04    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [10510/18659]  eta: 2:56:51    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [10520/18659]  eta: 2:56:38    time: 1.3063  data: 0.0002  max mem: 7129\n",
      "Test:  [10530/18659]  eta: 2:56:25    time: 1.3060  data: 0.0002  max mem: 7129\n",
      "Test:  [10540/18659]  eta: 2:56:12    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [10550/18659]  eta: 2:55:59    time: 1.2980  data: 0.0002  max mem: 7129\n",
      "Test:  [10560/18659]  eta: 2:55:46    time: 1.3000  data: 0.0002  max mem: 7129\n",
      "Test:  [10570/18659]  eta: 2:55:33    time: 1.3001  data: 0.0002  max mem: 7129\n",
      "Test:  [10580/18659]  eta: 2:55:20    time: 1.3156  data: 0.0002  max mem: 7129\n",
      "Test:  [10590/18659]  eta: 2:55:07    time: 1.3143  data: 0.0002  max mem: 7129\n",
      "Test:  [10600/18659]  eta: 2:54:54    time: 1.2956  data: 0.0002  max mem: 7129\n",
      "Test:  [10610/18659]  eta: 2:54:41    time: 1.3117  data: 0.0002  max mem: 7129\n",
      "Test:  [10620/18659]  eta: 2:54:28    time: 1.3158  data: 0.0002  max mem: 7129\n",
      "Test:  [10630/18659]  eta: 2:54:15    time: 1.3012  data: 0.0002  max mem: 7129\n",
      "Test:  [10640/18659]  eta: 2:54:02    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [10650/18659]  eta: 2:53:49    time: 1.2983  data: 0.0000  max mem: 7129\n",
      "Test:  [10660/18659]  eta: 2:53:36    time: 1.2983  data: 0.0000  max mem: 7129\n",
      "Test:  [10670/18659]  eta: 2:53:23    time: 1.2984  data: 0.0001  max mem: 7129\n",
      "Test:  [10680/18659]  eta: 2:53:10    time: 1.2984  data: 0.0001  max mem: 7129\n",
      "Test:  [10690/18659]  eta: 2:52:57    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [10700/18659]  eta: 2:52:44    time: 1.2985  data: 0.0003  max mem: 7129\n",
      "Test:  [10710/18659]  eta: 2:52:31    time: 1.2984  data: 0.0002  max mem: 7129\n",
      "Test:  [10720/18659]  eta: 2:52:18    time: 1.2983  data: 0.0002  max mem: 7129\n",
      "Test:  [10730/18659]  eta: 2:52:05    time: 1.2984  data: 0.0001  max mem: 7129\n",
      "Test:  [10740/18659]  eta: 2:51:52    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [10750/18659]  eta: 2:51:39    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [10760/18659]  eta: 2:51:26    time: 1.2980  data: 0.0002  max mem: 7129\n",
      "Test:  [10770/18659]  eta: 2:51:13    time: 1.2986  data: 0.0001  max mem: 7129\n",
      "Test:  [10780/18659]  eta: 2:51:00    time: 1.2990  data: 0.0000  max mem: 7129\n",
      "Test:  [10790/18659]  eta: 2:50:47    time: 1.2988  data: 0.0002  max mem: 7129\n",
      "Test:  [10800/18659]  eta: 2:50:33    time: 1.2986  data: 0.0002  max mem: 7129\n",
      "Test:  [10810/18659]  eta: 2:50:20    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [10820/18659]  eta: 2:50:07    time: 1.2984  data: 0.0003  max mem: 7129\n",
      "Test:  [10830/18659]  eta: 2:49:54    time: 1.2981  data: 0.0002  max mem: 7129\n",
      "Test:  [10840/18659]  eta: 2:49:41    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [10850/18659]  eta: 2:49:28    time: 1.3094  data: 0.0002  max mem: 7129\n",
      "Test:  [10860/18659]  eta: 2:49:15    time: 1.3083  data: 0.0002  max mem: 7129\n",
      "Test:  [10870/18659]  eta: 2:49:02    time: 1.2960  data: 0.0001  max mem: 7129\n",
      "Test:  [10880/18659]  eta: 2:48:49    time: 1.2956  data: 0.0001  max mem: 7129\n",
      "Test:  [10890/18659]  eta: 2:48:36    time: 1.2954  data: 0.0000  max mem: 7129\n",
      "Test:  [10900/18659]  eta: 2:48:23    time: 1.2953  data: 0.0001  max mem: 7129\n",
      "Test:  [10910/18659]  eta: 2:48:10    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [10920/18659]  eta: 2:47:57    time: 1.2948  data: 0.0003  max mem: 7129\n",
      "Test:  [10930/18659]  eta: 2:47:44    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [10940/18659]  eta: 2:47:31    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [10950/18659]  eta: 2:47:18    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [10960/18659]  eta: 2:47:05    time: 1.2992  data: 0.0001  max mem: 7129\n",
      "Test:  [10970/18659]  eta: 2:46:52    time: 1.2995  data: 0.0002  max mem: 7129\n",
      "Test:  [10980/18659]  eta: 2:46:39    time: 1.2992  data: 0.0001  max mem: 7129\n",
      "Test:  [10990/18659]  eta: 2:46:26    time: 1.3175  data: 0.0002  max mem: 7129\n",
      "Test:  [11000/18659]  eta: 2:46:13    time: 1.3159  data: 0.0002  max mem: 7129\n",
      "Test:  [11010/18659]  eta: 2:46:00    time: 1.2989  data: 0.0002  max mem: 7129\n",
      "Test:  [11020/18659]  eta: 2:45:47    time: 1.2980  data: 0.0002  max mem: 7129\n",
      "Test:  [11030/18659]  eta: 2:45:34    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [11040/18659]  eta: 2:45:21    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [11050/18659]  eta: 2:45:08    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [11060/18659]  eta: 2:44:54    time: 1.2987  data: 0.0002  max mem: 7129\n",
      "Test:  [11070/18659]  eta: 2:44:42    time: 1.3092  data: 0.0003  max mem: 7129\n",
      "Test:  [11080/18659]  eta: 2:44:29    time: 1.3088  data: 0.0003  max mem: 7129\n",
      "Test:  [11090/18659]  eta: 2:44:15    time: 1.2981  data: 0.0002  max mem: 7129\n",
      "Test:  [11100/18659]  eta: 2:44:02    time: 1.2983  data: 0.0002  max mem: 7129\n",
      "Test:  [11110/18659]  eta: 2:43:49    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [11120/18659]  eta: 2:43:36    time: 1.2981  data: 0.0002  max mem: 7129\n",
      "Test:  [11130/18659]  eta: 2:43:23    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [11140/18659]  eta: 2:43:10    time: 1.2989  data: 0.0000  max mem: 7129\n",
      "Test:  [11150/18659]  eta: 2:42:57    time: 1.2992  data: 0.0000  max mem: 7129\n",
      "Test:  [11160/18659]  eta: 2:42:44    time: 1.2986  data: 0.0000  max mem: 7129\n",
      "Test:  [11170/18659]  eta: 2:42:31    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [11180/18659]  eta: 2:42:18    time: 1.2982  data: 0.0003  max mem: 7129\n",
      "Test:  [11190/18659]  eta: 2:42:05    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [11200/18659]  eta: 2:41:52    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [11210/18659]  eta: 2:41:39    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [11220/18659]  eta: 2:41:26    time: 1.2982  data: 0.0003  max mem: 7129\n",
      "Test:  [11230/18659]  eta: 2:41:13    time: 1.2983  data: 0.0003  max mem: 7129\n",
      "Test:  [11240/18659]  eta: 2:41:00    time: 1.3032  data: 0.0001  max mem: 7129\n",
      "Test:  [11250/18659]  eta: 2:40:47    time: 1.3082  data: 0.0000  max mem: 7129\n",
      "Test:  [11260/18659]  eta: 2:40:34    time: 1.3035  data: 0.0000  max mem: 7129\n",
      "Test:  [11270/18659]  eta: 2:40:21    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [11280/18659]  eta: 2:40:08    time: 1.3015  data: 0.0002  max mem: 7129\n",
      "Test:  [11290/18659]  eta: 2:39:55    time: 1.3074  data: 0.0002  max mem: 7129\n",
      "Test:  [11300/18659]  eta: 2:39:42    time: 1.3064  data: 0.0001  max mem: 7129\n",
      "Test:  [11310/18659]  eta: 2:39:29    time: 1.2988  data: 0.0001  max mem: 7129\n",
      "Test:  [11320/18659]  eta: 2:39:16    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [11330/18659]  eta: 2:39:03    time: 1.2958  data: 0.0001  max mem: 7129\n",
      "Test:  [11340/18659]  eta: 2:38:50    time: 1.2958  data: 0.0001  max mem: 7129\n",
      "Test:  [11350/18659]  eta: 2:38:36    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [11360/18659]  eta: 2:38:23    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [11370/18659]  eta: 2:38:10    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [11380/18659]  eta: 2:37:57    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [11390/18659]  eta: 2:37:44    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [11400/18659]  eta: 2:37:31    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [11410/18659]  eta: 2:37:18    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [11420/18659]  eta: 2:37:05    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [11430/18659]  eta: 2:36:52    time: 1.3002  data: 0.0000  max mem: 7129\n",
      "Test:  [11440/18659]  eta: 2:36:39    time: 1.3001  data: 0.0001  max mem: 7129\n",
      "Test:  [11450/18659]  eta: 2:36:26    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [11460/18659]  eta: 2:36:13    time: 1.2948  data: 0.0000  max mem: 7129\n",
      "Test:  [11470/18659]  eta: 2:36:00    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [11480/18659]  eta: 2:35:47    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [11490/18659]  eta: 2:35:34    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [11500/18659]  eta: 2:35:21    time: 1.2947  data: 0.0003  max mem: 7129\n",
      "Test:  [11510/18659]  eta: 2:35:08    time: 1.3155  data: 0.0002  max mem: 7129\n",
      "Test:  [11520/18659]  eta: 2:34:55    time: 1.3171  data: 0.0001  max mem: 7129\n",
      "Test:  [11530/18659]  eta: 2:34:42    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [11540/18659]  eta: 2:34:29    time: 1.2981  data: 0.0000  max mem: 7129\n",
      "Test:  [11550/18659]  eta: 2:34:16    time: 1.3003  data: 0.0001  max mem: 7129\n",
      "Test:  [11560/18659]  eta: 2:34:03    time: 1.3004  data: 0.0001  max mem: 7129\n",
      "Test:  [11570/18659]  eta: 2:33:49    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [11580/18659]  eta: 2:33:36    time: 1.2980  data: 0.0002  max mem: 7129\n",
      "Test:  [11590/18659]  eta: 2:33:23    time: 1.2979  data: 0.0002  max mem: 7129\n",
      "Test:  [11600/18659]  eta: 2:33:10    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [11610/18659]  eta: 2:32:57    time: 1.2989  data: 0.0001  max mem: 7129\n",
      "Test:  [11620/18659]  eta: 2:32:44    time: 1.2991  data: 0.0001  max mem: 7129\n",
      "Test:  [11630/18659]  eta: 2:32:31    time: 1.2986  data: 0.0000  max mem: 7129\n",
      "Test:  [11640/18659]  eta: 2:32:18    time: 1.3114  data: 0.0000  max mem: 7129\n",
      "Test:  [11650/18659]  eta: 2:32:05    time: 1.3114  data: 0.0001  max mem: 7129\n",
      "Test:  [11660/18659]  eta: 2:31:52    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [11670/18659]  eta: 2:31:39    time: 1.2981  data: 0.0000  max mem: 7129\n",
      "Test:  [11680/18659]  eta: 2:31:26    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [11690/18659]  eta: 2:31:13    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [11700/18659]  eta: 2:31:00    time: 1.3016  data: 0.0002  max mem: 7129\n",
      "Test:  [11710/18659]  eta: 2:30:47    time: 1.3017  data: 0.0001  max mem: 7129\n",
      "Test:  [11720/18659]  eta: 2:30:34    time: 1.2983  data: 0.0002  max mem: 7129\n",
      "Test:  [11730/18659]  eta: 2:30:21    time: 1.3066  data: 0.0001  max mem: 7129\n",
      "Test:  [11740/18659]  eta: 2:30:08    time: 1.3108  data: 0.0000  max mem: 7129\n",
      "Test:  [11750/18659]  eta: 2:29:55    time: 1.3008  data: 0.0001  max mem: 7129\n",
      "Test:  [11760/18659]  eta: 2:29:42    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [11770/18659]  eta: 2:29:29    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [11780/18659]  eta: 2:29:16    time: 1.2952  data: 0.0000  max mem: 7129\n",
      "Test:  [11790/18659]  eta: 2:29:03    time: 1.2952  data: 0.0000  max mem: 7129\n",
      "Test:  [11800/18659]  eta: 2:28:50    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [11810/18659]  eta: 2:28:37    time: 1.3056  data: 0.0001  max mem: 7129\n",
      "Test:  [11820/18659]  eta: 2:28:24    time: 1.3056  data: 0.0003  max mem: 7129\n",
      "Test:  [11830/18659]  eta: 2:28:11    time: 1.2948  data: 0.0003  max mem: 7129\n",
      "Test:  [11840/18659]  eta: 2:27:58    time: 1.2948  data: 0.0002  max mem: 7129\n",
      "Test:  [11850/18659]  eta: 2:27:45    time: 1.2949  data: 0.0000  max mem: 7129\n",
      "Test:  [11860/18659]  eta: 2:27:31    time: 1.2948  data: 0.0002  max mem: 7129\n",
      "Test:  [11870/18659]  eta: 2:27:19    time: 1.3165  data: 0.0004  max mem: 7129\n",
      "Test:  [11880/18659]  eta: 2:27:06    time: 1.3193  data: 0.0004  max mem: 7129\n",
      "Test:  [11890/18659]  eta: 2:26:53    time: 1.2994  data: 0.0002  max mem: 7129\n",
      "Test:  [11900/18659]  eta: 2:26:40    time: 1.2994  data: 0.0001  max mem: 7129\n",
      "Test:  [11910/18659]  eta: 2:26:27    time: 1.2998  data: 0.0002  max mem: 7129\n",
      "Test:  [11920/18659]  eta: 2:26:14    time: 1.2987  data: 0.0004  max mem: 7129\n",
      "Test:  [11930/18659]  eta: 2:26:00    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [11940/18659]  eta: 2:25:47    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [11950/18659]  eta: 2:25:34    time: 1.2997  data: 0.0001  max mem: 7129\n",
      "Test:  [11960/18659]  eta: 2:25:21    time: 1.2997  data: 0.0001  max mem: 7129\n",
      "Test:  [11970/18659]  eta: 2:25:08    time: 1.2986  data: 0.0001  max mem: 7129\n",
      "Test:  [11980/18659]  eta: 2:24:55    time: 1.2988  data: 0.0002  max mem: 7129\n",
      "Test:  [11990/18659]  eta: 2:24:42    time: 1.3035  data: 0.0001  max mem: 7129\n",
      "Test:  [12000/18659]  eta: 2:24:29    time: 1.3132  data: 0.0000  max mem: 7129\n",
      "Test:  [12010/18659]  eta: 2:24:16    time: 1.3082  data: 0.0000  max mem: 7129\n",
      "Test:  [12020/18659]  eta: 2:24:03    time: 1.2983  data: 0.0000  max mem: 7129\n",
      "Test:  [12030/18659]  eta: 2:23:50    time: 1.2985  data: 0.0000  max mem: 7129\n",
      "Test:  [12040/18659]  eta: 2:23:37    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [12050/18659]  eta: 2:23:24    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [12060/18659]  eta: 2:23:11    time: 1.3101  data: 0.0001  max mem: 7129\n",
      "Test:  [12070/18659]  eta: 2:22:58    time: 1.3101  data: 0.0002  max mem: 7129\n",
      "Test:  [12080/18659]  eta: 2:22:45    time: 1.3150  data: 0.0002  max mem: 7129\n",
      "Test:  [12090/18659]  eta: 2:22:32    time: 1.3148  data: 0.0001  max mem: 7129\n",
      "Test:  [12100/18659]  eta: 2:22:19    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [12110/18659]  eta: 2:22:06    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [12120/18659]  eta: 2:21:53    time: 1.2983  data: 0.0002  max mem: 7129\n",
      "Test:  [12130/18659]  eta: 2:21:40    time: 1.2981  data: 0.0002  max mem: 7129\n",
      "Test:  [12140/18659]  eta: 2:21:27    time: 1.2982  data: 0.0002  max mem: 7129\n",
      "Test:  [12150/18659]  eta: 2:21:14    time: 1.2988  data: 0.0001  max mem: 7129\n",
      "Test:  [12160/18659]  eta: 2:21:01    time: 1.2989  data: 0.0002  max mem: 7129\n",
      "Test:  [12170/18659]  eta: 2:20:48    time: 1.2983  data: 0.0002  max mem: 7129\n",
      "Test:  [12180/18659]  eta: 2:20:35    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [12190/18659]  eta: 2:20:22    time: 1.2975  data: 0.0000  max mem: 7129\n",
      "Test:  [12200/18659]  eta: 2:20:09    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [12210/18659]  eta: 2:19:56    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [12220/18659]  eta: 2:19:43    time: 1.3197  data: 0.0002  max mem: 7129\n",
      "Test:  [12230/18659]  eta: 2:19:30    time: 1.3199  data: 0.0001  max mem: 7129\n",
      "Test:  [12240/18659]  eta: 2:19:17    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [12250/18659]  eta: 2:19:04    time: 1.2981  data: 0.0001  max mem: 7129\n",
      "Test:  [12260/18659]  eta: 2:18:51    time: 1.2983  data: 0.0000  max mem: 7129\n",
      "Test:  [12270/18659]  eta: 2:18:38    time: 1.2977  data: 0.0000  max mem: 7129\n",
      "Test:  [12280/18659]  eta: 2:18:25    time: 1.2977  data: 0.0000  max mem: 7129\n",
      "Test:  [12290/18659]  eta: 2:18:12    time: 1.2978  data: 0.0000  max mem: 7129\n",
      "Test:  [12300/18659]  eta: 2:17:59    time: 1.2975  data: 0.0002  max mem: 7129\n",
      "Test:  [12310/18659]  eta: 2:17:46    time: 1.2975  data: 0.0002  max mem: 7129\n",
      "Test:  [12320/18659]  eta: 2:17:33    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [12330/18659]  eta: 2:17:20    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [12340/18659]  eta: 2:17:07    time: 1.2978  data: 0.0000  max mem: 7129\n",
      "Test:  [12350/18659]  eta: 2:16:53    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [12360/18659]  eta: 2:16:41    time: 1.3065  data: 0.0002  max mem: 7129\n",
      "Test:  [12370/18659]  eta: 2:16:28    time: 1.3094  data: 0.0002  max mem: 7129\n",
      "Test:  [12380/18659]  eta: 2:16:15    time: 1.3538  data: 0.0002  max mem: 7129\n",
      "Test:  [12390/18659]  eta: 2:16:02    time: 1.3495  data: 0.0002  max mem: 7129\n",
      "Test:  [12400/18659]  eta: 2:15:49    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [12410/18659]  eta: 2:15:36    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [12420/18659]  eta: 2:15:23    time: 1.2949  data: 0.0003  max mem: 7129\n",
      "Test:  [12430/18659]  eta: 2:15:10    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [12440/18659]  eta: 2:14:57    time: 1.2957  data: 0.0001  max mem: 7129\n",
      "Test:  [12450/18659]  eta: 2:14:44    time: 1.2957  data: 0.0002  max mem: 7129\n",
      "Test:  [12460/18659]  eta: 2:14:31    time: 1.2952  data: 0.0003  max mem: 7129\n",
      "Test:  [12470/18659]  eta: 2:14:18    time: 1.2947  data: 0.0003  max mem: 7129\n",
      "Test:  [12480/18659]  eta: 2:14:04    time: 1.2948  data: 0.0003  max mem: 7129\n",
      "Test:  [12490/18659]  eta: 2:13:51    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [12500/18659]  eta: 2:13:38    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [12510/18659]  eta: 2:13:25    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [12520/18659]  eta: 2:13:12    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [12530/18659]  eta: 2:12:59    time: 1.2984  data: 0.0000  max mem: 7129\n",
      "Test:  [12540/18659]  eta: 2:12:46    time: 1.2982  data: 0.0001  max mem: 7129\n",
      "Test:  [12550/18659]  eta: 2:12:33    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [12560/18659]  eta: 2:12:20    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [12570/18659]  eta: 2:12:07    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [12580/18659]  eta: 2:11:54    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [12590/18659]  eta: 2:11:41    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [12600/18659]  eta: 2:11:28    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [12610/18659]  eta: 2:11:15    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [12620/18659]  eta: 2:11:02    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [12630/18659]  eta: 2:10:49    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [12640/18659]  eta: 2:10:36    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [12650/18659]  eta: 2:10:23    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [12660/18659]  eta: 2:10:10    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [12670/18659]  eta: 2:09:56    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [12680/18659]  eta: 2:09:43    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [12690/18659]  eta: 2:09:30    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [12700/18659]  eta: 2:09:17    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [12710/18659]  eta: 2:09:04    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [12720/18659]  eta: 2:08:51    time: 1.2956  data: 0.0001  max mem: 7129\n",
      "Test:  [12730/18659]  eta: 2:08:38    time: 1.2954  data: 0.0001  max mem: 7129\n",
      "Test:  [12740/18659]  eta: 2:08:25    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [12750/18659]  eta: 2:08:12    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [12760/18659]  eta: 2:07:59    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [12770/18659]  eta: 2:07:46    time: 1.2979  data: 0.0000  max mem: 7129\n",
      "Test:  [12780/18659]  eta: 2:07:33    time: 1.3044  data: 0.0000  max mem: 7129\n",
      "Test:  [12790/18659]  eta: 2:07:20    time: 1.3046  data: 0.0001  max mem: 7129\n",
      "Test:  [12800/18659]  eta: 2:07:07    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [12810/18659]  eta: 2:06:54    time: 1.2996  data: 0.0001  max mem: 7129\n",
      "Test:  [12820/18659]  eta: 2:06:41    time: 1.2990  data: 0.0001  max mem: 7129\n",
      "Test:  [12830/18659]  eta: 2:06:28    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [12840/18659]  eta: 2:06:15    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [12850/18659]  eta: 2:06:02    time: 1.2941  data: 0.0003  max mem: 7129\n",
      "Test:  [12860/18659]  eta: 2:05:49    time: 1.2944  data: 0.0004  max mem: 7129\n",
      "Test:  [12870/18659]  eta: 2:05:36    time: 1.2946  data: 0.0003  max mem: 7129\n",
      "Test:  [12880/18659]  eta: 2:05:23    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [12890/18659]  eta: 2:05:10    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [12900/18659]  eta: 2:04:56    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [12910/18659]  eta: 2:04:43    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [12920/18659]  eta: 2:04:30    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [12930/18659]  eta: 2:04:17    time: 1.2947  data: 0.0003  max mem: 7129\n",
      "Test:  [12940/18659]  eta: 2:04:04    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [12950/18659]  eta: 2:03:51    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [12960/18659]  eta: 2:03:38    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [12970/18659]  eta: 2:03:25    time: 1.2947  data: 0.0000  max mem: 7129\n",
      "Test:  [12980/18659]  eta: 2:03:12    time: 1.2947  data: 0.0000  max mem: 7129\n",
      "Test:  [12990/18659]  eta: 2:02:59    time: 1.2953  data: 0.0001  max mem: 7129\n",
      "Test:  [13000/18659]  eta: 2:02:46    time: 1.2955  data: 0.0002  max mem: 7129\n",
      "Test:  [13010/18659]  eta: 2:02:33    time: 1.2948  data: 0.0002  max mem: 7129\n",
      "Test:  [13020/18659]  eta: 2:02:20    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [13030/18659]  eta: 2:02:07    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [13040/18659]  eta: 2:01:54    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [13050/18659]  eta: 2:01:41    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [13060/18659]  eta: 2:01:28    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [13070/18659]  eta: 2:01:15    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [13080/18659]  eta: 2:01:02    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [13090/18659]  eta: 2:00:49    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [13100/18659]  eta: 2:00:36    time: 1.2997  data: 0.0001  max mem: 7129\n",
      "Test:  [13110/18659]  eta: 2:00:22    time: 1.2993  data: 0.0001  max mem: 7129\n",
      "Test:  [13120/18659]  eta: 2:00:09    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [13130/18659]  eta: 1:59:56    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [13140/18659]  eta: 1:59:43    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [13150/18659]  eta: 1:59:30    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [13160/18659]  eta: 1:59:17    time: 1.3002  data: 0.0002  max mem: 7129\n",
      "Test:  [13170/18659]  eta: 1:59:04    time: 1.3004  data: 0.0001  max mem: 7129\n",
      "Test:  [13180/18659]  eta: 1:58:51    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [13190/18659]  eta: 1:58:38    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [13200/18659]  eta: 1:58:25    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [13210/18659]  eta: 1:58:12    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [13220/18659]  eta: 1:57:59    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [13230/18659]  eta: 1:57:46    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [13240/18659]  eta: 1:57:33    time: 1.3041  data: 0.0001  max mem: 7129\n",
      "Test:  [13250/18659]  eta: 1:57:20    time: 1.3043  data: 0.0001  max mem: 7129\n",
      "Test:  [13260/18659]  eta: 1:57:07    time: 1.2950  data: 0.0000  max mem: 7129\n",
      "Test:  [13270/18659]  eta: 1:56:54    time: 1.2954  data: 0.0000  max mem: 7129\n",
      "Test:  [13280/18659]  eta: 1:56:41    time: 1.2953  data: 0.0001  max mem: 7129\n",
      "Test:  [13290/18659]  eta: 1:56:28    time: 1.2948  data: 0.0002  max mem: 7129\n",
      "Test:  [13300/18659]  eta: 1:56:15    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [13310/18659]  eta: 1:56:02    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [13320/18659]  eta: 1:55:49    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [13330/18659]  eta: 1:55:36    time: 1.2944  data: 0.0000  max mem: 7129\n",
      "Test:  [13340/18659]  eta: 1:55:23    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [13350/18659]  eta: 1:55:10    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [13360/18659]  eta: 1:54:56    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [13370/18659]  eta: 1:54:43    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [13380/18659]  eta: 1:54:30    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [13390/18659]  eta: 1:54:17    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [13400/18659]  eta: 1:54:04    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [13410/18659]  eta: 1:53:51    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [13420/18659]  eta: 1:53:38    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [13430/18659]  eta: 1:53:25    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [13440/18659]  eta: 1:53:12    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [13450/18659]  eta: 1:52:59    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [13460/18659]  eta: 1:52:46    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [13470/18659]  eta: 1:52:33    time: 1.2995  data: 0.0001  max mem: 7129\n",
      "Test:  [13480/18659]  eta: 1:52:20    time: 1.2992  data: 0.0001  max mem: 7129\n",
      "Test:  [13490/18659]  eta: 1:52:07    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [13500/18659]  eta: 1:51:54    time: 1.2942  data: 0.0003  max mem: 7129\n",
      "Test:  [13510/18659]  eta: 1:51:41    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [13520/18659]  eta: 1:51:28    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [13530/18659]  eta: 1:51:15    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [13540/18659]  eta: 1:51:02    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [13550/18659]  eta: 1:50:49    time: 1.2950  data: 0.0000  max mem: 7129\n",
      "Test:  [13560/18659]  eta: 1:50:36    time: 1.3049  data: 0.0000  max mem: 7129\n",
      "Test:  [13570/18659]  eta: 1:50:23    time: 1.3046  data: 0.0001  max mem: 7129\n",
      "Test:  [13580/18659]  eta: 1:50:10    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [13590/18659]  eta: 1:49:57    time: 1.2944  data: 0.0003  max mem: 7129\n",
      "Test:  [13600/18659]  eta: 1:49:44    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [13610/18659]  eta: 1:49:31    time: 1.3349  data: 0.0001  max mem: 7129\n",
      "Test:  [13620/18659]  eta: 1:49:18    time: 1.3361  data: 0.0000  max mem: 7129\n",
      "Test:  [13630/18659]  eta: 1:49:05    time: 1.2969  data: 0.0000  max mem: 7129\n",
      "Test:  [13640/18659]  eta: 1:48:52    time: 1.2975  data: 0.0000  max mem: 7129\n",
      "Test:  [13650/18659]  eta: 1:48:39    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [13660/18659]  eta: 1:48:26    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [13670/18659]  eta: 1:48:13    time: 1.2971  data: 0.0001  max mem: 7129\n",
      "Test:  [13680/18659]  eta: 1:48:00    time: 1.3046  data: 0.0001  max mem: 7129\n",
      "Test:  [13690/18659]  eta: 1:47:47    time: 1.3045  data: 0.0002  max mem: 7129\n",
      "Test:  [13700/18659]  eta: 1:47:34    time: 1.2973  data: 0.0002  max mem: 7129\n",
      "Test:  [13710/18659]  eta: 1:47:21    time: 1.2972  data: 0.0001  max mem: 7129\n",
      "Test:  [13720/18659]  eta: 1:47:08    time: 1.2971  data: 0.0001  max mem: 7129\n",
      "Test:  [13730/18659]  eta: 1:46:55    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [13740/18659]  eta: 1:46:42    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [13750/18659]  eta: 1:46:28    time: 1.2972  data: 0.0000  max mem: 7129\n",
      "Test:  [13760/18659]  eta: 1:46:15    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [13770/18659]  eta: 1:46:02    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [13780/18659]  eta: 1:45:49    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [13790/18659]  eta: 1:45:36    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [13800/18659]  eta: 1:45:23    time: 1.2995  data: 0.0001  max mem: 7129\n",
      "Test:  [13810/18659]  eta: 1:45:10    time: 1.2994  data: 0.0001  max mem: 7129\n",
      "Test:  [13820/18659]  eta: 1:44:57    time: 1.3168  data: 0.0002  max mem: 7129\n",
      "Test:  [13830/18659]  eta: 1:44:44    time: 1.3170  data: 0.0002  max mem: 7129\n",
      "Test:  [13840/18659]  eta: 1:44:31    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [13850/18659]  eta: 1:44:18    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [13860/18659]  eta: 1:44:05    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [13870/18659]  eta: 1:43:52    time: 1.2973  data: 0.0002  max mem: 7129\n",
      "Test:  [13880/18659]  eta: 1:43:39    time: 1.3022  data: 0.0002  max mem: 7129\n",
      "Test:  [13890/18659]  eta: 1:43:26    time: 1.3022  data: 0.0001  max mem: 7129\n",
      "Test:  [13900/18659]  eta: 1:43:13    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [13910/18659]  eta: 1:43:00    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [13920/18659]  eta: 1:42:47    time: 1.2980  data: 0.0002  max mem: 7129\n",
      "Test:  [13930/18659]  eta: 1:42:34    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [13940/18659]  eta: 1:42:21    time: 1.2975  data: 0.0002  max mem: 7129\n",
      "Test:  [13950/18659]  eta: 1:42:08    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [13960/18659]  eta: 1:41:55    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [13970/18659]  eta: 1:41:42    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [13980/18659]  eta: 1:41:29    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [13990/18659]  eta: 1:41:16    time: 1.2975  data: 0.0002  max mem: 7129\n",
      "Test:  [14000/18659]  eta: 1:41:03    time: 1.2975  data: 0.0002  max mem: 7129\n",
      "Test:  [14010/18659]  eta: 1:40:50    time: 1.3010  data: 0.0001  max mem: 7129\n",
      "Test:  [14020/18659]  eta: 1:40:37    time: 1.3009  data: 0.0001  max mem: 7129\n",
      "Test:  [14030/18659]  eta: 1:40:24    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [14040/18659]  eta: 1:40:11    time: 1.3079  data: 0.0001  max mem: 7129\n",
      "Test:  [14050/18659]  eta: 1:39:58    time: 1.3080  data: 0.0001  max mem: 7129\n",
      "Test:  [14060/18659]  eta: 1:39:45    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [14070/18659]  eta: 1:39:32    time: 1.2971  data: 0.0002  max mem: 7129\n",
      "Test:  [14080/18659]  eta: 1:39:19    time: 1.2971  data: 0.0002  max mem: 7129\n",
      "Test:  [14090/18659]  eta: 1:39:06    time: 1.3097  data: 0.0002  max mem: 7129\n",
      "Test:  [14100/18659]  eta: 1:38:53    time: 1.3340  data: 0.0002  max mem: 7129\n",
      "Test:  [14110/18659]  eta: 1:38:40    time: 1.3404  data: 0.0001  max mem: 7129\n",
      "Test:  [14120/18659]  eta: 1:38:27    time: 1.3150  data: 0.0000  max mem: 7129\n",
      "Test:  [14130/18659]  eta: 1:38:14    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [14140/18659]  eta: 1:38:01    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [14150/18659]  eta: 1:37:48    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [14160/18659]  eta: 1:37:35    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [14170/18659]  eta: 1:37:22    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [14180/18659]  eta: 1:37:09    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [14190/18659]  eta: 1:36:56    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [14200/18659]  eta: 1:36:43    time: 1.2985  data: 0.0001  max mem: 7129\n",
      "Test:  [14210/18659]  eta: 1:36:30    time: 1.2979  data: 0.0002  max mem: 7129\n",
      "Test:  [14220/18659]  eta: 1:36:17    time: 1.2993  data: 0.0004  max mem: 7129\n",
      "Test:  [14230/18659]  eta: 1:36:04    time: 1.3049  data: 0.0002  max mem: 7129\n",
      "Test:  [14240/18659]  eta: 1:35:51    time: 1.2997  data: 0.0000  max mem: 7129\n",
      "Test:  [14250/18659]  eta: 1:35:38    time: 1.3237  data: 0.0001  max mem: 7129\n",
      "Test:  [14260/18659]  eta: 1:35:25    time: 1.3271  data: 0.0001  max mem: 7129\n",
      "Test:  [14270/18659]  eta: 1:35:12    time: 1.3009  data: 0.0001  max mem: 7129\n",
      "Test:  [14280/18659]  eta: 1:34:59    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [14290/18659]  eta: 1:34:46    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [14300/18659]  eta: 1:34:33    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [14310/18659]  eta: 1:34:20    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [14320/18659]  eta: 1:34:07    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [14330/18659]  eta: 1:33:54    time: 1.3060  data: 0.0001  max mem: 7129\n",
      "Test:  [14340/18659]  eta: 1:33:41    time: 1.3058  data: 0.0002  max mem: 7129\n",
      "Test:  [14350/18659]  eta: 1:33:28    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [14360/18659]  eta: 1:33:15    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [14370/18659]  eta: 1:33:02    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [14380/18659]  eta: 1:32:48    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [14390/18659]  eta: 1:32:35    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [14400/18659]  eta: 1:32:22    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [14410/18659]  eta: 1:32:09    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [14420/18659]  eta: 1:31:56    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [14430/18659]  eta: 1:31:43    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [14440/18659]  eta: 1:31:30    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [14450/18659]  eta: 1:31:17    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [14460/18659]  eta: 1:31:04    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [14470/18659]  eta: 1:30:51    time: 1.2949  data: 0.0000  max mem: 7129\n",
      "Test:  [14480/18659]  eta: 1:30:38    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [14490/18659]  eta: 1:30:25    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [14500/18659]  eta: 1:30:12    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [14510/18659]  eta: 1:29:59    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [14520/18659]  eta: 1:29:46    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [14530/18659]  eta: 1:29:33    time: 1.3001  data: 0.0002  max mem: 7129\n",
      "Test:  [14540/18659]  eta: 1:29:20    time: 1.3002  data: 0.0001  max mem: 7129\n",
      "Test:  [14550/18659]  eta: 1:29:07    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [14560/18659]  eta: 1:28:54    time: 1.2943  data: 0.0003  max mem: 7129\n",
      "Test:  [14570/18659]  eta: 1:28:41    time: 1.3027  data: 0.0001  max mem: 7129\n",
      "Test:  [14580/18659]  eta: 1:28:28    time: 1.3026  data: 0.0000  max mem: 7129\n",
      "Test:  [14590/18659]  eta: 1:28:15    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [14600/18659]  eta: 1:28:02    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [14610/18659]  eta: 1:27:49    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [14620/18659]  eta: 1:27:36    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [14630/18659]  eta: 1:27:23    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [14640/18659]  eta: 1:27:10    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [14650/18659]  eta: 1:26:57    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [14660/18659]  eta: 1:26:44    time: 1.2953  data: 0.0003  max mem: 7129\n",
      "Test:  [14670/18659]  eta: 1:26:31    time: 1.2949  data: 0.0003  max mem: 7129\n",
      "Test:  [14680/18659]  eta: 1:26:18    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [14690/18659]  eta: 1:26:05    time: 1.2941  data: 0.0003  max mem: 7129\n",
      "Test:  [14700/18659]  eta: 1:25:51    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [14710/18659]  eta: 1:25:38    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [14720/18659]  eta: 1:25:25    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [14730/18659]  eta: 1:25:12    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [14740/18659]  eta: 1:24:59    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [14750/18659]  eta: 1:24:46    time: 1.3173  data: 0.0001  max mem: 7129\n",
      "Test:  [14760/18659]  eta: 1:24:33    time: 1.3265  data: 0.0000  max mem: 7129\n",
      "Test:  [14770/18659]  eta: 1:24:20    time: 1.3134  data: 0.0001  max mem: 7129\n",
      "Test:  [14780/18659]  eta: 1:24:07    time: 1.3039  data: 0.0001  max mem: 7129\n",
      "Test:  [14790/18659]  eta: 1:23:54    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [14800/18659]  eta: 1:23:41    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [14810/18659]  eta: 1:23:28    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [14820/18659]  eta: 1:23:15    time: 1.2937  data: 0.0001  max mem: 7129\n",
      "Test:  [14830/18659]  eta: 1:23:02    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [14840/18659]  eta: 1:22:49    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [14850/18659]  eta: 1:22:36    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [14860/18659]  eta: 1:22:23    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [14870/18659]  eta: 1:22:10    time: 1.2999  data: 0.0001  max mem: 7129\n",
      "Test:  [14880/18659]  eta: 1:21:57    time: 1.2998  data: 0.0001  max mem: 7129\n",
      "Test:  [14890/18659]  eta: 1:21:44    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [14900/18659]  eta: 1:21:31    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [14910/18659]  eta: 1:21:18    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [14920/18659]  eta: 1:21:05    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [14930/18659]  eta: 1:20:52    time: 1.3138  data: 0.0001  max mem: 7129\n",
      "Test:  [14940/18659]  eta: 1:20:39    time: 1.3109  data: 0.0001  max mem: 7129\n",
      "Test:  [14950/18659]  eta: 1:20:26    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [14960/18659]  eta: 1:20:13    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [14970/18659]  eta: 1:20:00    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [14980/18659]  eta: 1:19:47    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [14990/18659]  eta: 1:19:34    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [15000/18659]  eta: 1:19:21    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [15010/18659]  eta: 1:19:08    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [15020/18659]  eta: 1:18:55    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [15030/18659]  eta: 1:18:42    time: 1.2953  data: 0.0001  max mem: 7129\n",
      "Test:  [15040/18659]  eta: 1:18:29    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [15050/18659]  eta: 1:18:16    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [15060/18659]  eta: 1:18:03    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [15070/18659]  eta: 1:17:50    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [15080/18659]  eta: 1:17:37    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [15090/18659]  eta: 1:17:24    time: 1.2986  data: 0.0001  max mem: 7129\n",
      "Test:  [15100/18659]  eta: 1:17:11    time: 1.3020  data: 0.0001  max mem: 7129\n",
      "Test:  [15110/18659]  eta: 1:16:58    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [15120/18659]  eta: 1:16:45    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [15130/18659]  eta: 1:16:32    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [15140/18659]  eta: 1:16:19    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [15150/18659]  eta: 1:16:06    time: 1.2979  data: 0.0000  max mem: 7129\n",
      "Test:  [15160/18659]  eta: 1:15:53    time: 1.3011  data: 0.0001  max mem: 7129\n",
      "Test:  [15170/18659]  eta: 1:15:39    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [15180/18659]  eta: 1:15:26    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [15190/18659]  eta: 1:15:13    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [15200/18659]  eta: 1:15:00    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [15210/18659]  eta: 1:14:47    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [15220/18659]  eta: 1:14:34    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [15230/18659]  eta: 1:14:21    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [15240/18659]  eta: 1:14:08    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [15250/18659]  eta: 1:13:55    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [15260/18659]  eta: 1:13:42    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [15270/18659]  eta: 1:13:29    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [15280/18659]  eta: 1:13:16    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [15290/18659]  eta: 1:13:03    time: 1.3148  data: 0.0003  max mem: 7129\n",
      "Test:  [15300/18659]  eta: 1:12:50    time: 1.3164  data: 0.0002  max mem: 7129\n",
      "Test:  [15310/18659]  eta: 1:12:37    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [15320/18659]  eta: 1:12:24    time: 1.2972  data: 0.0001  max mem: 7129\n",
      "Test:  [15330/18659]  eta: 1:12:11    time: 1.2969  data: 0.0004  max mem: 7129\n",
      "Test:  [15340/18659]  eta: 1:11:58    time: 1.2972  data: 0.0003  max mem: 7129\n",
      "Test:  [15350/18659]  eta: 1:11:45    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [15360/18659]  eta: 1:11:32    time: 1.2974  data: 0.0002  max mem: 7129\n",
      "Test:  [15370/18659]  eta: 1:11:19    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [15380/18659]  eta: 1:11:06    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [15390/18659]  eta: 1:10:53    time: 1.2977  data: 0.0002  max mem: 7129\n",
      "Test:  [15400/18659]  eta: 1:10:40    time: 1.2975  data: 0.0002  max mem: 7129\n",
      "Test:  [15410/18659]  eta: 1:10:27    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [15420/18659]  eta: 1:10:14    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [15430/18659]  eta: 1:10:01    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [15440/18659]  eta: 1:09:48    time: 1.2974  data: 0.0002  max mem: 7129\n",
      "Test:  [15450/18659]  eta: 1:09:35    time: 1.2972  data: 0.0001  max mem: 7129\n",
      "Test:  [15460/18659]  eta: 1:09:22    time: 1.2971  data: 0.0001  max mem: 7129\n",
      "Test:  [15470/18659]  eta: 1:09:09    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [15480/18659]  eta: 1:08:56    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [15490/18659]  eta: 1:08:43    time: 1.2979  data: 0.0002  max mem: 7129\n",
      "Test:  [15500/18659]  eta: 1:08:30    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [15510/18659]  eta: 1:08:17    time: 1.2974  data: 0.0002  max mem: 7129\n",
      "Test:  [15520/18659]  eta: 1:08:04    time: 1.3030  data: 0.0002  max mem: 7129\n",
      "Test:  [15530/18659]  eta: 1:07:51    time: 1.3054  data: 0.0002  max mem: 7129\n",
      "Test:  [15540/18659]  eta: 1:07:38    time: 1.2984  data: 0.0001  max mem: 7129\n",
      "Test:  [15550/18659]  eta: 1:07:25    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [15560/18659]  eta: 1:07:12    time: 1.2946  data: 0.0003  max mem: 7129\n",
      "Test:  [15570/18659]  eta: 1:06:59    time: 1.2948  data: 0.0003  max mem: 7129\n",
      "Test:  [15580/18659]  eta: 1:06:46    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [15590/18659]  eta: 1:06:33    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [15600/18659]  eta: 1:06:20    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [15610/18659]  eta: 1:06:07    time: 1.2945  data: 0.0003  max mem: 7129\n",
      "Test:  [15620/18659]  eta: 1:05:54    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [15630/18659]  eta: 1:05:41    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [15640/18659]  eta: 1:05:28    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [15650/18659]  eta: 1:05:15    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [15660/18659]  eta: 1:05:01    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [15670/18659]  eta: 1:04:49    time: 1.3100  data: 0.0002  max mem: 7129\n",
      "Test:  [15680/18659]  eta: 1:04:36    time: 1.3147  data: 0.0002  max mem: 7129\n",
      "Test:  [15690/18659]  eta: 1:04:22    time: 1.3000  data: 0.0001  max mem: 7129\n",
      "Test:  [15700/18659]  eta: 1:04:09    time: 1.3019  data: 0.0003  max mem: 7129\n",
      "Test:  [15710/18659]  eta: 1:03:56    time: 1.3048  data: 0.0003  max mem: 7129\n",
      "Test:  [15720/18659]  eta: 1:03:43    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [15730/18659]  eta: 1:03:30    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [15740/18659]  eta: 1:03:17    time: 1.2941  data: 0.0003  max mem: 7129\n",
      "Test:  [15750/18659]  eta: 1:03:04    time: 1.2983  data: 0.0001  max mem: 7129\n",
      "Test:  [15760/18659]  eta: 1:02:51    time: 1.2993  data: 0.0002  max mem: 7129\n",
      "Test:  [15770/18659]  eta: 1:02:38    time: 1.2954  data: 0.0002  max mem: 7129\n",
      "Test:  [15780/18659]  eta: 1:02:25    time: 1.2948  data: 0.0000  max mem: 7129\n",
      "Test:  [15790/18659]  eta: 1:02:12    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [15800/18659]  eta: 1:01:59    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [15810/18659]  eta: 1:01:46    time: 1.2941  data: 0.0003  max mem: 7129\n",
      "Test:  [15820/18659]  eta: 1:01:33    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [15830/18659]  eta: 1:01:20    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [15840/18659]  eta: 1:01:07    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [15850/18659]  eta: 1:00:54    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [15860/18659]  eta: 1:00:41    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [15870/18659]  eta: 1:00:28    time: 1.3058  data: 0.0001  max mem: 7129\n",
      "Test:  [15880/18659]  eta: 1:00:15    time: 1.3021  data: 0.0002  max mem: 7129\n",
      "Test:  [15890/18659]  eta: 1:00:02    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [15900/18659]  eta: 0:59:49    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [15910/18659]  eta: 0:59:36    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [15920/18659]  eta: 0:59:23    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [15930/18659]  eta: 0:59:10    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [15940/18659]  eta: 0:58:57    time: 1.3001  data: 0.0002  max mem: 7129\n",
      "Test:  [15950/18659]  eta: 0:58:44    time: 1.3053  data: 0.0001  max mem: 7129\n",
      "Test:  [15960/18659]  eta: 0:58:31    time: 1.2997  data: 0.0002  max mem: 7129\n",
      "Test:  [15970/18659]  eta: 0:58:18    time: 1.3171  data: 0.0000  max mem: 7129\n",
      "Test:  [15980/18659]  eta: 0:58:05    time: 1.3227  data: 0.0000  max mem: 7129\n",
      "Test:  [15990/18659]  eta: 0:57:52    time: 1.3018  data: 0.0001  max mem: 7129\n",
      "Test:  [16000/18659]  eta: 0:57:39    time: 1.2975  data: 0.0001  max mem: 7129\n",
      "Test:  [16010/18659]  eta: 0:57:26    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [16020/18659]  eta: 0:57:13    time: 1.2973  data: 0.0001  max mem: 7129\n",
      "Test:  [16030/18659]  eta: 0:57:00    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [16040/18659]  eta: 0:56:47    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [16050/18659]  eta: 0:56:34    time: 1.2982  data: 0.0000  max mem: 7129\n",
      "Test:  [16060/18659]  eta: 0:56:21    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [16070/18659]  eta: 0:56:08    time: 1.2972  data: 0.0002  max mem: 7129\n",
      "Test:  [16080/18659]  eta: 0:55:55    time: 1.2971  data: 0.0002  max mem: 7129\n",
      "Test:  [16090/18659]  eta: 0:55:42    time: 1.2971  data: 0.0002  max mem: 7129\n",
      "Test:  [16100/18659]  eta: 0:55:29    time: 1.2971  data: 0.0002  max mem: 7129\n",
      "Test:  [16110/18659]  eta: 0:55:16    time: 1.2995  data: 0.0001  max mem: 7129\n",
      "Test:  [16120/18659]  eta: 0:55:03    time: 1.2996  data: 0.0001  max mem: 7129\n",
      "Test:  [16130/18659]  eta: 0:54:50    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [16140/18659]  eta: 0:54:37    time: 1.2980  data: 0.0001  max mem: 7129\n",
      "Test:  [16150/18659]  eta: 0:54:24    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [16160/18659]  eta: 0:54:11    time: 1.2972  data: 0.0001  max mem: 7129\n",
      "Test:  [16170/18659]  eta: 0:53:58    time: 1.2971  data: 0.0001  max mem: 7129\n",
      "Test:  [16180/18659]  eta: 0:53:45    time: 1.2972  data: 0.0002  max mem: 7129\n",
      "Test:  [16190/18659]  eta: 0:53:32    time: 1.2974  data: 0.0002  max mem: 7129\n",
      "Test:  [16200/18659]  eta: 0:53:19    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [16210/18659]  eta: 0:53:06    time: 1.2973  data: 0.0000  max mem: 7129\n",
      "Test:  [16220/18659]  eta: 0:52:53    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [16230/18659]  eta: 0:52:40    time: 1.2972  data: 0.0002  max mem: 7129\n",
      "Test:  [16240/18659]  eta: 0:52:27    time: 1.3056  data: 0.0002  max mem: 7129\n",
      "Test:  [16250/18659]  eta: 0:52:14    time: 1.3145  data: 0.0001  max mem: 7129\n",
      "Test:  [16260/18659]  eta: 0:52:01    time: 1.3062  data: 0.0001  max mem: 7129\n",
      "Test:  [16270/18659]  eta: 0:51:48    time: 1.2958  data: 0.0002  max mem: 7129\n",
      "Test:  [16280/18659]  eta: 0:51:35    time: 1.2945  data: 0.0003  max mem: 7129\n",
      "Test:  [16290/18659]  eta: 0:51:22    time: 1.3040  data: 0.0003  max mem: 7129\n",
      "Test:  [16300/18659]  eta: 0:51:09    time: 1.3043  data: 0.0002  max mem: 7129\n",
      "Test:  [16310/18659]  eta: 0:50:56    time: 1.2953  data: 0.0001  max mem: 7129\n",
      "Test:  [16320/18659]  eta: 0:50:43    time: 1.2954  data: 0.0000  max mem: 7129\n",
      "Test:  [16330/18659]  eta: 0:50:30    time: 1.2955  data: 0.0001  max mem: 7129\n",
      "Test:  [16340/18659]  eta: 0:50:17    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [16350/18659]  eta: 0:50:04    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [16360/18659]  eta: 0:49:51    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [16370/18659]  eta: 0:49:37    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [16380/18659]  eta: 0:49:24    time: 1.2944  data: 0.0000  max mem: 7129\n",
      "Test:  [16390/18659]  eta: 0:49:11    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [16400/18659]  eta: 0:48:58    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [16410/18659]  eta: 0:48:45    time: 1.2952  data: 0.0000  max mem: 7129\n",
      "Test:  [16420/18659]  eta: 0:48:32    time: 1.2953  data: 0.0001  max mem: 7129\n",
      "Test:  [16430/18659]  eta: 0:48:19    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [16440/18659]  eta: 0:48:06    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [16450/18659]  eta: 0:47:53    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [16460/18659]  eta: 0:47:40    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [16470/18659]  eta: 0:47:27    time: 1.2945  data: 0.0002  max mem: 7129\n",
      "Test:  [16480/18659]  eta: 0:47:14    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [16490/18659]  eta: 0:47:01    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [16500/18659]  eta: 0:46:48    time: 1.2944  data: 0.0000  max mem: 7129\n",
      "Test:  [16510/18659]  eta: 0:46:35    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16520/18659]  eta: 0:46:22    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16530/18659]  eta: 0:46:09    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [16540/18659]  eta: 0:45:56    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [16550/18659]  eta: 0:45:43    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [16560/18659]  eta: 0:45:30    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [16570/18659]  eta: 0:45:17    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [16580/18659]  eta: 0:45:04    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [16590/18659]  eta: 0:44:51    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [16600/18659]  eta: 0:44:38    time: 1.2947  data: 0.0000  max mem: 7129\n",
      "Test:  [16610/18659]  eta: 0:44:25    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16620/18659]  eta: 0:44:12    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [16630/18659]  eta: 0:43:59    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [16640/18659]  eta: 0:43:46    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [16650/18659]  eta: 0:43:33    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [16660/18659]  eta: 0:43:20    time: 1.2944  data: 0.0000  max mem: 7129\n",
      "Test:  [16670/18659]  eta: 0:43:07    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16680/18659]  eta: 0:42:54    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [16690/18659]  eta: 0:42:41    time: 1.2950  data: 0.0000  max mem: 7129\n",
      "Test:  [16700/18659]  eta: 0:42:28    time: 1.2950  data: 0.0001  max mem: 7129\n",
      "Test:  [16710/18659]  eta: 0:42:15    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [16720/18659]  eta: 0:42:02    time: 1.2977  data: 0.0001  max mem: 7129\n",
      "Test:  [16730/18659]  eta: 0:41:49    time: 1.2976  data: 0.0001  max mem: 7129\n",
      "Test:  [16740/18659]  eta: 0:41:36    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [16750/18659]  eta: 0:41:23    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [16760/18659]  eta: 0:41:10    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [16770/18659]  eta: 0:40:57    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16780/18659]  eta: 0:40:44    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16790/18659]  eta: 0:40:31    time: 1.2945  data: 0.0000  max mem: 7129\n",
      "Test:  [16800/18659]  eta: 0:40:18    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [16810/18659]  eta: 0:40:05    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16820/18659]  eta: 0:39:52    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [16830/18659]  eta: 0:39:39    time: 1.2941  data: 0.0003  max mem: 7129\n",
      "Test:  [16840/18659]  eta: 0:39:26    time: 1.2942  data: 0.0004  max mem: 7129\n",
      "Test:  [16850/18659]  eta: 0:39:13    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [16860/18659]  eta: 0:39:00    time: 1.2941  data: 0.0000  max mem: 7129\n",
      "Test:  [16870/18659]  eta: 0:38:47    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [16880/18659]  eta: 0:38:34    time: 1.2986  data: 0.0003  max mem: 7129\n",
      "Test:  [16890/18659]  eta: 0:38:21    time: 1.2984  data: 0.0003  max mem: 7129\n",
      "Test:  [16900/18659]  eta: 0:38:08    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16910/18659]  eta: 0:37:55    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [16920/18659]  eta: 0:37:42    time: 1.2941  data: 0.0003  max mem: 7129\n",
      "Test:  [16930/18659]  eta: 0:37:29    time: 1.2940  data: 0.0003  max mem: 7129\n",
      "Test:  [16940/18659]  eta: 0:37:16    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [16950/18659]  eta: 0:37:03    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [16960/18659]  eta: 0:36:50    time: 1.2946  data: 0.0000  max mem: 7129\n",
      "Test:  [16970/18659]  eta: 0:36:37    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [16980/18659]  eta: 0:36:23    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [16990/18659]  eta: 0:36:10    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [17000/18659]  eta: 0:35:57    time: 1.2939  data: 0.0003  max mem: 7129\n",
      "Test:  [17010/18659]  eta: 0:35:44    time: 1.2939  data: 0.0004  max mem: 7129\n",
      "Test:  [17020/18659]  eta: 0:35:31    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [17030/18659]  eta: 0:35:18    time: 1.2941  data: 0.0000  max mem: 7129\n",
      "Test:  [17040/18659]  eta: 0:35:05    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [17050/18659]  eta: 0:34:52    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [17060/18659]  eta: 0:34:39    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [17070/18659]  eta: 0:34:26    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [17080/18659]  eta: 0:34:13    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [17090/18659]  eta: 0:34:00    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17100/18659]  eta: 0:33:47    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [17110/18659]  eta: 0:33:34    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [17120/18659]  eta: 0:33:21    time: 1.2942  data: 0.0003  max mem: 7129\n",
      "Test:  [17130/18659]  eta: 0:33:08    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [17140/18659]  eta: 0:32:55    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [17150/18659]  eta: 0:32:42    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [17160/18659]  eta: 0:32:29    time: 1.3134  data: 0.0002  max mem: 7129\n",
      "Test:  [17170/18659]  eta: 0:32:16    time: 1.3128  data: 0.0002  max mem: 7129\n",
      "Test:  [17180/18659]  eta: 0:32:03    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [17190/18659]  eta: 0:31:50    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [17200/18659]  eta: 0:31:37    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [17210/18659]  eta: 0:31:24    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [17220/18659]  eta: 0:31:11    time: 1.2939  data: 0.0003  max mem: 7129\n",
      "Test:  [17230/18659]  eta: 0:30:58    time: 1.2938  data: 0.0001  max mem: 7129\n",
      "Test:  [17240/18659]  eta: 0:30:45    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17250/18659]  eta: 0:30:32    time: 1.2949  data: 0.0001  max mem: 7129\n",
      "Test:  [17260/18659]  eta: 0:30:19    time: 1.2995  data: 0.0001  max mem: 7129\n",
      "Test:  [17270/18659]  eta: 0:30:06    time: 1.3042  data: 0.0001  max mem: 7129\n",
      "Test:  [17280/18659]  eta: 0:29:53    time: 1.2991  data: 0.0002  max mem: 7129\n",
      "Test:  [17290/18659]  eta: 0:29:40    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [17300/18659]  eta: 0:29:27    time: 1.2937  data: 0.0001  max mem: 7129\n",
      "Test:  [17310/18659]  eta: 0:29:14    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [17320/18659]  eta: 0:29:01    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [17330/18659]  eta: 0:28:48    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [17340/18659]  eta: 0:28:35    time: 1.2939  data: 0.0002  max mem: 7129\n",
      "Test:  [17350/18659]  eta: 0:28:22    time: 1.2972  data: 0.0001  max mem: 7129\n",
      "Test:  [17360/18659]  eta: 0:28:09    time: 1.2976  data: 0.0002  max mem: 7129\n",
      "Test:  [17370/18659]  eta: 0:27:56    time: 1.2985  data: 0.0002  max mem: 7129\n",
      "Test:  [17380/18659]  eta: 0:27:43    time: 1.2984  data: 0.0001  max mem: 7129\n",
      "Test:  [17390/18659]  eta: 0:27:30    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [17400/18659]  eta: 0:27:17    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [17410/18659]  eta: 0:27:04    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [17420/18659]  eta: 0:26:51    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [17430/18659]  eta: 0:26:38    time: 1.2951  data: 0.0001  max mem: 7129\n",
      "Test:  [17440/18659]  eta: 0:26:25    time: 1.2952  data: 0.0001  max mem: 7129\n",
      "Test:  [17450/18659]  eta: 0:26:12    time: 1.2948  data: 0.0001  max mem: 7129\n",
      "Test:  [17460/18659]  eta: 0:25:59    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [17470/18659]  eta: 0:25:46    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [17480/18659]  eta: 0:25:33    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [17490/18659]  eta: 0:25:20    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [17500/18659]  eta: 0:25:07    time: 1.2941  data: 0.0000  max mem: 7129\n",
      "Test:  [17510/18659]  eta: 0:24:54    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17520/18659]  eta: 0:24:41    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [17530/18659]  eta: 0:24:28    time: 1.3104  data: 0.0001  max mem: 7129\n",
      "Test:  [17540/18659]  eta: 0:24:15    time: 1.3099  data: 0.0001  max mem: 7129\n",
      "Test:  [17550/18659]  eta: 0:24:02    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [17560/18659]  eta: 0:23:49    time: 1.3040  data: 0.0002  max mem: 7129\n",
      "Test:  [17570/18659]  eta: 0:23:36    time: 1.3040  data: 0.0002  max mem: 7129\n",
      "Test:  [17580/18659]  eta: 0:23:23    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [17590/18659]  eta: 0:23:10    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [17600/18659]  eta: 0:22:57    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [17610/18659]  eta: 0:22:44    time: 1.2941  data: 0.0000  max mem: 7129\n",
      "Test:  [17620/18659]  eta: 0:22:31    time: 1.2944  data: 0.0000  max mem: 7129\n",
      "Test:  [17630/18659]  eta: 0:22:18    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [17640/18659]  eta: 0:22:05    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17650/18659]  eta: 0:21:52    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [17660/18659]  eta: 0:21:39    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [17670/18659]  eta: 0:21:26    time: 1.3023  data: 0.0001  max mem: 7129\n",
      "Test:  [17680/18659]  eta: 0:21:13    time: 1.3024  data: 0.0001  max mem: 7129\n",
      "Test:  [17690/18659]  eta: 0:21:00    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [17700/18659]  eta: 0:20:47    time: 1.2945  data: 0.0001  max mem: 7129\n",
      "Test:  [17710/18659]  eta: 0:20:34    time: 1.2947  data: 0.0001  max mem: 7129\n",
      "Test:  [17720/18659]  eta: 0:20:21    time: 1.2950  data: 0.0002  max mem: 7129\n",
      "Test:  [17730/18659]  eta: 0:20:08    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [17740/18659]  eta: 0:19:55    time: 1.2941  data: 0.0003  max mem: 7129\n",
      "Test:  [17750/18659]  eta: 0:19:42    time: 1.2940  data: 0.0003  max mem: 7129\n",
      "Test:  [17760/18659]  eta: 0:19:29    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [17770/18659]  eta: 0:19:16    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [17780/18659]  eta: 0:19:03    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [17790/18659]  eta: 0:18:50    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17800/18659]  eta: 0:18:37    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [17810/18659]  eta: 0:18:24    time: 1.2947  data: 0.0002  max mem: 7129\n",
      "Test:  [17820/18659]  eta: 0:18:11    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [17830/18659]  eta: 0:17:58    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [17840/18659]  eta: 0:17:45    time: 1.2998  data: 0.0001  max mem: 7129\n",
      "Test:  [17850/18659]  eta: 0:17:32    time: 1.2996  data: 0.0001  max mem: 7129\n",
      "Test:  [17860/18659]  eta: 0:17:19    time: 1.2938  data: 0.0001  max mem: 7129\n",
      "Test:  [17870/18659]  eta: 0:17:06    time: 1.2940  data: 0.0000  max mem: 7129\n",
      "Test:  [17880/18659]  eta: 0:16:53    time: 1.2943  data: 0.0000  max mem: 7129\n",
      "Test:  [17890/18659]  eta: 0:16:40    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17900/18659]  eta: 0:16:27    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17910/18659]  eta: 0:16:14    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [17920/18659]  eta: 0:16:01    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17930/18659]  eta: 0:15:48    time: 1.2942  data: 0.0000  max mem: 7129\n",
      "Test:  [17940/18659]  eta: 0:15:35    time: 1.3025  data: 0.0001  max mem: 7129\n",
      "Test:  [17950/18659]  eta: 0:15:22    time: 1.3026  data: 0.0002  max mem: 7129\n",
      "Test:  [17960/18659]  eta: 0:15:09    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [17970/18659]  eta: 0:14:56    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [17980/18659]  eta: 0:14:43    time: 1.2947  data: 0.0000  max mem: 7129\n",
      "Test:  [17990/18659]  eta: 0:14:30    time: 1.2951  data: 0.0002  max mem: 7129\n",
      "Test:  [18000/18659]  eta: 0:14:17    time: 1.2949  data: 0.0003  max mem: 7129\n",
      "Test:  [18010/18659]  eta: 0:14:04    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [18020/18659]  eta: 0:13:51    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [18030/18659]  eta: 0:13:38    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [18040/18659]  eta: 0:13:25    time: 1.3048  data: 0.0002  max mem: 7129\n",
      "Test:  [18050/18659]  eta: 0:13:11    time: 1.3048  data: 0.0003  max mem: 7129\n",
      "Test:  [18060/18659]  eta: 0:12:58    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [18070/18659]  eta: 0:12:45    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [18080/18659]  eta: 0:12:32    time: 1.2949  data: 0.0002  max mem: 7129\n",
      "Test:  [18090/18659]  eta: 0:12:19    time: 1.2946  data: 0.0003  max mem: 7129\n",
      "Test:  [18100/18659]  eta: 0:12:06    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [18110/18659]  eta: 0:11:53    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [18120/18659]  eta: 0:11:40    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [18130/18659]  eta: 0:11:27    time: 1.2938  data: 0.0001  max mem: 7129\n",
      "Test:  [18140/18659]  eta: 0:11:14    time: 1.2938  data: 0.0000  max mem: 7129\n",
      "Test:  [18150/18659]  eta: 0:11:01    time: 1.2989  data: 0.0000  max mem: 7129\n",
      "Test:  [18160/18659]  eta: 0:10:48    time: 1.2991  data: 0.0001  max mem: 7129\n",
      "Test:  [18170/18659]  eta: 0:10:35    time: 1.2943  data: 0.0001  max mem: 7129\n",
      "Test:  [18180/18659]  eta: 0:10:22    time: 1.2944  data: 0.0002  max mem: 7129\n",
      "Test:  [18190/18659]  eta: 0:10:09    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [18200/18659]  eta: 0:09:56    time: 1.2974  data: 0.0002  max mem: 7129\n",
      "Test:  [18210/18659]  eta: 0:09:43    time: 1.2974  data: 0.0002  max mem: 7129\n",
      "Test:  [18220/18659]  eta: 0:09:30    time: 1.2941  data: 0.0002  max mem: 7129\n",
      "Test:  [18230/18659]  eta: 0:09:17    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [18240/18659]  eta: 0:09:04    time: 1.2938  data: 0.0001  max mem: 7129\n",
      "Test:  [18250/18659]  eta: 0:08:51    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [18260/18659]  eta: 0:08:38    time: 1.2946  data: 0.0002  max mem: 7129\n",
      "Test:  [18270/18659]  eta: 0:08:25    time: 1.2949  data: 0.0003  max mem: 7129\n",
      "Test:  [18280/18659]  eta: 0:08:12    time: 1.2945  data: 0.0003  max mem: 7129\n",
      "Test:  [18290/18659]  eta: 0:07:59    time: 1.2941  data: 0.0001  max mem: 7129\n",
      "Test:  [18300/18659]  eta: 0:07:46    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [18310/18659]  eta: 0:07:33    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [18320/18659]  eta: 0:07:20    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [18330/18659]  eta: 0:07:07    time: 1.2936  data: 0.0002  max mem: 7129\n",
      "Test:  [18340/18659]  eta: 0:06:54    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [18350/18659]  eta: 0:06:41    time: 1.2944  data: 0.0001  max mem: 7129\n",
      "Test:  [18360/18659]  eta: 0:06:28    time: 1.2946  data: 0.0001  max mem: 7129\n",
      "Test:  [18370/18659]  eta: 0:06:15    time: 1.2943  data: 0.0002  max mem: 7129\n",
      "Test:  [18380/18659]  eta: 0:06:02    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [18390/18659]  eta: 0:05:49    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [18400/18659]  eta: 0:05:36    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [18410/18659]  eta: 0:05:23    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [18420/18659]  eta: 0:05:10    time: 1.2937  data: 0.0003  max mem: 7129\n",
      "Test:  [18430/18659]  eta: 0:04:57    time: 1.2938  data: 0.0002  max mem: 7129\n",
      "Test:  [18440/18659]  eta: 0:04:44    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [18450/18659]  eta: 0:04:31    time: 1.2942  data: 0.0001  max mem: 7129\n",
      "Test:  [18460/18659]  eta: 0:04:18    time: 1.2942  data: 0.0002  max mem: 7129\n",
      "Test:  [18470/18659]  eta: 0:04:05    time: 1.2940  data: 0.0001  max mem: 7129\n",
      "Test:  [18480/18659]  eta: 0:03:52    time: 1.2939  data: 0.0000  max mem: 7129\n",
      "Test:  [18490/18659]  eta: 0:03:39    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [18500/18659]  eta: 0:03:26    time: 1.2939  data: 0.0002  max mem: 7129\n",
      "Test:  [18510/18659]  eta: 0:03:13    time: 1.2939  data: 0.0001  max mem: 7129\n",
      "Test:  [18520/18659]  eta: 0:03:00    time: 1.2940  data: 0.0002  max mem: 7129\n",
      "Test:  [18530/18659]  eta: 0:02:47    time: 1.3144  data: 0.0002  max mem: 7129\n",
      "Test:  [18540/18659]  eta: 0:02:34    time: 1.3209  data: 0.0001  max mem: 7129\n",
      "Test:  [18550/18659]  eta: 0:02:21    time: 1.3022  data: 0.0000  max mem: 7129\n",
      "Test:  [18560/18659]  eta: 0:02:08    time: 1.2971  data: 0.0001  max mem: 7129\n",
      "Test:  [18570/18659]  eta: 0:01:55    time: 1.3028  data: 0.0001  max mem: 7129\n",
      "Test:  [18580/18659]  eta: 0:01:42    time: 1.3097  data: 0.0001  max mem: 7129\n",
      "Test:  [18590/18659]  eta: 0:01:29    time: 1.3039  data: 0.0001  max mem: 7129\n",
      "Test:  [18600/18659]  eta: 0:01:16    time: 1.2970  data: 0.0001  max mem: 7129\n",
      "Test:  [18610/18659]  eta: 0:01:03    time: 1.2969  data: 0.0001  max mem: 7129\n",
      "Test:  [18620/18659]  eta: 0:00:50    time: 1.2969  data: 0.0001  max mem: 7129\n",
      "Test:  [18630/18659]  eta: 0:00:37    time: 1.2974  data: 0.0001  max mem: 7129\n",
      "Test:  [18640/18659]  eta: 0:00:24    time: 1.2979  data: 0.0001  max mem: 7129\n",
      "Test:  [18650/18659]  eta: 0:00:11    time: 1.2978  data: 0.0001  max mem: 7129\n",
      "Test:  [18658/18659]  eta: 0:00:01    time: 1.2424  data: 0.0001  max mem: 7129\n",
      "Test: Total time: 6:44:30 (1.3007 s / it)\n"
     ]
    }
   ],
   "source": [
    "%run run_beit3_finetuning.py \\\n",
    "        --model beit3_large_patch16_480 \\\n",
    "        --input_size 480 \\\n",
    "        --task vqav2 \\\n",
    "        --batch_size 16 \\\n",
    "        --sentencepiece_model ./model/beit3.spm \\\n",
    "        --finetune ./model/beit3_large_indomain_patch16_768_vgqaaug_vqa.zip \\\n",
    "        --data_path ./dataset/ \\\n",
    "        --output_dir ./output/ \\\n",
    "        --eval\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BEiT-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
