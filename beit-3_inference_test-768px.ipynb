{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not all images have caption annotations\n",
      "82783 82774 82774\n",
      "Write ./dataset/vqa.train.jsonl with 434867 items !\n",
      "not all images have caption annotations\n",
      "40504 40503 40503\n",
      "Write ./dataset/vqa.val.jsonl with 210051 items !\n",
      "all images have caption annotations\n",
      "81434 81434 81434\n",
      "Write ./dataset/vqa.test.jsonl with 447793 items !\n",
      "not all images have caption annotations\n",
      "81434 36807 36807\n",
      "Write ./dataset/vqa.test-dev.jsonl with 107394 items !\n",
      "Contains 40503 image and 210051 pairs for val set!\n",
      "Write ./dataset/vqa.trainable_val.jsonl with 204645 items !\n",
      "Write ./dataset/vqa.rest_val.jsonl with 5406 items !\n"
     ]
    }
   ],
   "source": [
    "from datasets import VQAv2Dataset\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer(\"./model//beit3.spm\")\n",
    "\n",
    "VQAv2Dataset.make_dataset_index(\n",
    "    data_path=\"./dataset/\",\n",
    "    tokenizer=tokenizer,\n",
    "    annotation_data_path=\"./dataset/vqa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Namespace(model='beit3_large_patch16_768', task='vqav2', input_size=768, drop_path=0.1, checkpoint_activations=None, sentencepiece_model='./model/beit3.spm', vocab_size=64010, num_max_bpe_tokens=64, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, lr=0.0005, layer_decay=0.9, task_head_lr_weight=0, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, batch_size=16, eval_batch_size=None, epochs=20, update_freq=1, save_ckpt_freq=5, randaug=False, train_interpolation='bicubic', finetune='./model/beit3_large_indomain_patch16_768_vgqaaug_vqa.zip', model_key='model|module', model_prefix='', data_path='./dataset/', output_dir='./output/', log_dir=None, device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=True, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', task_cache_path='./output/', nb_classes=1000, mixup=0, cutmix=0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, captioning_mask_prob=0.6, drop_worst_ratio=0.2, drop_worst_after=12000, num_beams=3, length_penalty=0.6, label_smoothing=0.1, enable_deepspeed=False, initial_scale_power=16, zero_stage=0, distributed=False)\n",
      "Load 434867 image-text pairs from ./dataset/vqa.train.jsonl. \n",
      "Load 204645 image-text pairs from ./dataset/vqa.trainable_val.jsonl. \n",
      "Load 5406 image-text pairs from ./dataset/vqa.rest_val.jsonl. \n",
      "model_config = beit3_large_patch16_768_vqav2\n",
      "Load ckpt from ./model/beit3_large_indomain_patch16_768_vgqaaug_vqa.zip\n",
      "Load state_dict by model_key = model\n",
      "Model = BEiT3ForVisualQuestionAnswering(\n",
      "  (beit3): BEiT3(\n",
      "    (text_embed): TextEmbedding(64010, 1024)\n",
      "    (vision_embed): VisionEmbedding(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "      (embed_positions): MutliwayEmbedding(\n",
      "        (A): PositionalEmbedding(2307, 1024)\n",
      "        (B): PositionalEmbedding(1024, 1024)\n",
      "      )\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.0)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.004347826086956522)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.008695652173913044)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.013043478260869566)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.017391304347826087)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.021739130434782608)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (6): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.026086956521739132)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (7): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.030434782608695653)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (8): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.034782608695652174)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (9): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.0391304347826087)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (10): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.043478260869565216)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (11): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.04782608695652174)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (12): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.052173913043478265)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (13): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.05652173913043478)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (14): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.06086956521739131)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (15): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.06521739130434782)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (16): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.06956521739130435)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (17): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.07391304347826087)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (18): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.0782608695652174)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (19): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.08260869565217391)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (20): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.08695652173913043)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (21): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.09130434782608696)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (22): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.09565217391304348)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (23): EncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (k_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (q_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (out_proj): MultiwayNetwork(\n",
      "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (inner_attn_ln): MultiwayNetwork(\n",
      "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (self_attn_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "          (drop_path): DropPath(p=0.1)\n",
      "          (ffn): MultiwayNetwork(\n",
      "            (A): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (B): FeedForwardNetwork(\n",
      "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MultiwayNetwork(\n",
      "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Linear(in_features=2048, out_features=3129, bias=True)\n",
      "  )\n",
      ")\n",
      "number of params: 684423225\n",
      "LR = 0.00050000\n",
      "Batch size = 16\n",
      "Update frequent = 1\n",
      "Number of training examples = 639512\n",
      "Number of training training per epoch = 39969\n",
      "Assigned values = [0.0717897987691853, 0.07976644307687256, 0.08862938119652507, 0.09847709021836118, 0.10941898913151242, 0.12157665459056935, 0.13508517176729928, 0.15009463529699918, 0.16677181699666577, 0.18530201888518416, 0.20589113209464907, 0.2287679245496101, 0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]\n",
      "Param groups = {\n",
      "  \"layer_0_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.text_embed.weight\",\n",
      "      \"beit3.vision_embed.mask_token\",\n",
      "      \"beit3.vision_embed.proj.weight\",\n",
      "      \"beit3.encoder.embed_positions.B.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.0717897987691853\n",
      "  },\n",
      "  \"layer_0_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.vision_embed.cls_token\",\n",
      "      \"beit3.vision_embed.proj.bias\",\n",
      "      \"beit3.encoder.embed_positions.A.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.0717897987691853\n",
      "  },\n",
      "  \"layer_1_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.07976644307687256\n",
      "  },\n",
      "  \"layer_1_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.07976644307687256\n",
      "  },\n",
      "  \"layer_2_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.08862938119652507\n",
      "  },\n",
      "  \"layer_2_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.08862938119652507\n",
      "  },\n",
      "  \"layer_3_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.09847709021836118\n",
      "  },\n",
      "  \"layer_3_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.09847709021836118\n",
      "  },\n",
      "  \"layer_4_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.10941898913151242\n",
      "  },\n",
      "  \"layer_4_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.10941898913151242\n",
      "  },\n",
      "  \"layer_5_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.12157665459056935\n",
      "  },\n",
      "  \"layer_5_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.12157665459056935\n",
      "  },\n",
      "  \"layer_6_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.13508517176729928\n",
      "  },\n",
      "  \"layer_6_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.13508517176729928\n",
      "  },\n",
      "  \"layer_7_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.15009463529699918\n",
      "  },\n",
      "  \"layer_7_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.15009463529699918\n",
      "  },\n",
      "  \"layer_8_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.16677181699666577\n",
      "  },\n",
      "  \"layer_8_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.16677181699666577\n",
      "  },\n",
      "  \"layer_9_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.18530201888518416\n",
      "  },\n",
      "  \"layer_9_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.18530201888518416\n",
      "  },\n",
      "  \"layer_10_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.20589113209464907\n",
      "  },\n",
      "  \"layer_10_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.20589113209464907\n",
      "  },\n",
      "  \"layer_11_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2287679245496101\n",
      "  },\n",
      "  \"layer_11_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2287679245496101\n",
      "  },\n",
      "  \"layer_12_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2541865828329001\n",
      "  },\n",
      "  \"layer_12_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2541865828329001\n",
      "  },\n",
      "  \"layer_13_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.12.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2824295364810001\n",
      "  },\n",
      "  \"layer_13_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.12.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.12.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.12.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.12.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.12.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.12.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.12.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.12.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2824295364810001\n",
      "  },\n",
      "  \"layer_14_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.13.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31381059609000006\n",
      "  },\n",
      "  \"layer_14_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.13.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.13.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.13.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.13.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.13.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.13.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.13.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.13.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31381059609000006\n",
      "  },\n",
      "  \"layer_15_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.14.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3486784401000001\n",
      "  },\n",
      "  \"layer_15_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.14.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.14.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.14.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.14.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.14.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.14.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.14.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.14.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3486784401000001\n",
      "  },\n",
      "  \"layer_16_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.15.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3874204890000001\n",
      "  },\n",
      "  \"layer_16_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.15.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.15.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.15.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.15.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.15.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.15.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.15.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.15.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3874204890000001\n",
      "  },\n",
      "  \"layer_17_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.16.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4304672100000001\n",
      "  },\n",
      "  \"layer_17_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.16.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.16.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.16.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.16.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.16.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.16.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.16.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.16.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4304672100000001\n",
      "  },\n",
      "  \"layer_18_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.17.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4782969000000001\n",
      "  },\n",
      "  \"layer_18_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.17.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.17.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.17.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.17.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.17.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.17.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.17.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.17.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4782969000000001\n",
      "  },\n",
      "  \"layer_19_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.18.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.531441\n",
      "  },\n",
      "  \"layer_19_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.18.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.18.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.18.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.18.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.18.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.18.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.18.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.18.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.531441\n",
      "  },\n",
      "  \"layer_20_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.19.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5904900000000001\n",
      "  },\n",
      "  \"layer_20_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.19.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.19.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.19.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.19.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.19.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.19.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.19.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.19.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5904900000000001\n",
      "  },\n",
      "  \"layer_21_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.20.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.6561\n",
      "  },\n",
      "  \"layer_21_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.20.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.20.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.20.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.20.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.20.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.20.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.20.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.20.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.6561\n",
      "  },\n",
      "  \"layer_22_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.21.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.7290000000000001\n",
      "  },\n",
      "  \"layer_22_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.21.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.21.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.21.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.21.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.21.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.21.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.21.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.21.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.7290000000000001\n",
      "  },\n",
      "  \"layer_23_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.22.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.81\n",
      "  },\n",
      "  \"layer_23_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.22.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.22.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.22.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.22.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.22.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.22.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.22.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.22.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.81\n",
      "  },\n",
      "  \"layer_24_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.23.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.9\n",
      "  },\n",
      "  \"layer_24_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.23.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.23.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.23.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.23.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.23.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.23.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.23.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.23.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.9\n",
      "  },\n",
      "  \"layer_25_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"pooler.norm.weight\",\n",
      "      \"pooler.norm.bias\",\n",
      "      \"pooler.dense.bias\",\n",
      "      \"head.0.bias\",\n",
      "      \"head.1.weight\",\n",
      "      \"head.1.bias\",\n",
      "      \"head.3.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"layer_25_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"pooler.dense.weight\",\n",
      "      \"head.0.weight\",\n",
      "      \"head.3.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "Set warmup steps = 199845\n",
      "Auto resume checkpoint: \n",
      "Load 447793 image-text pairs from ./dataset/vqa.test.jsonl. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Codes\\python_projects_windows\\VQA\\BEiT-3\\unilm\\beit3\\run_beit3_finetuning.py:448\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m opts\u001b[39m.\u001b[39moutput_dir:\n\u001b[0;32m    447\u001b[0m     Path(opts\u001b[39m.\u001b[39moutput_dir)\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 448\u001b[0m main(opts, ds_init)\n",
      "File \u001b[1;32mD:\\Codes\\python_projects_windows\\VQA\\BEiT-3\\unilm\\beit3\\run_beit3_finetuning.py:364\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args, ds_init)\u001b[0m\n\u001b[0;32m    362\u001b[0m     exit(\u001b[39m0\u001b[39m)\n\u001b[0;32m    363\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvqav2\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 364\u001b[0m     result, _ \u001b[39m=\u001b[39m evaluate(data_loader_test, model, device, task_handler)\n\u001b[0;32m    365\u001b[0m     utils\u001b[39m.\u001b[39mdump_predictions(args, result, \u001b[39m\"\u001b[39m\u001b[39mvqav2_test\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    366\u001b[0m     exit(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mD:\\Codes\\python_projects_windows\\VQA\\BEiT-3\\unilm\\beit3\\engine_for_finetuning.py:588\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(data_loader, model, device, handler)\u001b[0m\n\u001b[0;32m    585\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    586\u001b[0m handler\u001b[39m.\u001b[39mbefore_eval(metric_logger\u001b[39m=\u001b[39mmetric_logger, data_loader\u001b[39m=\u001b[39mdata_loader)\n\u001b[1;32m--> 588\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m metric_logger\u001b[39m.\u001b[39mlog_every(data_loader, \u001b[39m10\u001b[39m, header):\n\u001b[0;32m    589\u001b[0m     \u001b[39mfor\u001b[39;00m tensor_key \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    590\u001b[0m         data[tensor_key] \u001b[39m=\u001b[39m data[tensor_key]\u001b[39m.\u001b[39mto(device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Codes\\python_projects_windows\\VQA\\BEiT-3\\unilm\\beit3\\utils.py:163\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m    161\u001b[0m log_msg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelimiter\u001b[39m.\u001b[39mjoin(log_msg)\n\u001b[0;32m    162\u001b[0m MB \u001b[39m=\u001b[39m \u001b[39m1024.0\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024.0\u001b[39m\n\u001b[1;32m--> 163\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m    164\u001b[0m     data_time\u001b[39m.\u001b[39mupdate(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m end)\n\u001b[0;32m    165\u001b[0m     \u001b[39myield\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     95\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\joony\\mambaforge\\envs\\BEiT-3\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run run_beit3_finetuning.py \\\n",
    "        --model beit3_large_patch16_768 \\\n",
    "        --input_size 768 \\\n",
    "        --task vqav2 \\\n",
    "        --batch_size 16 \\\n",
    "        --sentencepiece_model ./model/beit3.spm \\\n",
    "        --finetune ./model/beit3_large_indomain_patch16_768_vgqaaug_vqa.zip \\\n",
    "        --data_path ./dataset/ \\\n",
    "        --output_dir ./output/ \\\n",
    "        --eval\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BEiT-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
