{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import utils\n",
    "import modeling_finetune\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "from custom_dataset import CustomVQADataset\n",
    "from timm.models import create_model\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "\n",
    "current_path = os.getcwd()\n",
    "custom_dataset_path = os.path.join(current_path, 'custom_dataset')\n",
    "jsonl_path = os.path.join(custom_dataset_path, '[google]custom.vqa.test.jsonl')\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(f'CUDA Available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터세트 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomVQADataset(jsonl_path=jsonl_path, img_size=768)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ckpt from d:\\Codes\\python_projects_windows\\VQA\\BEiT-3\\unilm\\beit3\\Custom_BEiT-3\\model\\beit3_large_indomain_patch16_768_vgqaaug_vqa.zip\n",
      "Load state_dict by model_key = model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BEiT3ForVisualQuestionAnswering(\n",
       "  (beit3): BEiT3(\n",
       "    (text_embed): TextEmbedding(64010, 1024)\n",
       "    (vision_embed): VisionEmbedding(\n",
       "      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "      (embed_positions): MutliwayEmbedding(\n",
       "        (A): PositionalEmbedding(2307, 1024)\n",
       "        (B): PositionalEmbedding(1024, 1024)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.0)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.004347826086956522)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.008695652173913044)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.013043478260869566)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.017391304347826087)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.021739130434782608)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.026086956521739132)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.030434782608695653)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.034782608695652174)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.0391304347826087)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.043478260869565216)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.04782608695652174)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (12): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.052173913043478265)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (13): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.05652173913043478)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (14): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.06086956521739131)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (15): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.06521739130434782)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (16): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.06956521739130435)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (17): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.07391304347826087)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (18): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.0782608695652174)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (19): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.08260869565217391)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (20): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.08695652173913043)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (21): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.09130434782608696)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (22): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.09565217391304348)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (23): EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (B): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path): DropPath(p=0.1)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Pooler(\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=2048, out_features=3129, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = 'beit3_large_patch16_768_vqav2'\n",
    "model = create_model(\n",
    "    model_config,\n",
    "    pretrained=False,\n",
    "    drop_path_rate=0.1,\n",
    "    vocab_size=64010\n",
    ")\n",
    "\n",
    "utils.load_model_and_may_interpolate(\n",
    "    ckpt_path=os.path.join(current_path, 'model', 'beit3_large_indomain_patch16_768_vgqaaug_vqa.zip'),\n",
    "    model=model,\n",
    "    model_key='model|module',\n",
    "    model_prefix=''\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24197e745e1543618bd96e244a1b5144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 2\n"
     ]
    }
   ],
   "source": [
    "save_jsonl_path = os.path.join(current_path, 'output', 'submit_[google].jsonl')\n",
    "\n",
    "tokenizer_path = os.path.join(current_path,'model','beit3.spm')\n",
    "default_tokenizer = XLMRobertaTokenizer(tokenizer_path)\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        progress = 0\n",
    "        counter = 0\n",
    "        for data in tqdm(test_loader):\n",
    "            # if counter == 4:\n",
    "            #     raise KeyError\n",
    "            if counter >= progress:\n",
    "                images = data['image'].to(device)\n",
    "                question = data['question'].to(device)\n",
    "                padding_mask = data['padding_mask'].to(device)\n",
    "\n",
    "                outputs = model(images, question, padding_mask)\n",
    "\n",
    "                _, pred = outputs.max(-1)\n",
    "                for index, x in enumerate(pred):\n",
    "                    preds.append({\"question\":default_tokenizer.decode(data['question'][index], skip_special_tokens= True), \"answer\":test_dataset.label2ans[x]})\n",
    "            \n",
    "            counter += 1\n",
    "    except:\n",
    "        print(f'Progress: {counter}')\n",
    "        for pred in preds:\n",
    "            # print(pred)\n",
    "            with open(save_jsonl_path, \"a\", encoding='utf-8') as f:\n",
    "                    f.write(json.dumps(pred, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json(path_or_buf=jsonl_path, lines=True, encoding='utf-8')\n",
    "answers_df = pd.read_json(path_or_buf=save_jsonl_path, lines=True, encoding='utf-8')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(400, 300))\n",
    "\n",
    "from matplotlib import font_manager, rc # 폰트 세팅을 위한 모듈 추가\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\" # 사용할 폰트명 경로 삽입\n",
    "font = font_manager.FontProperties(fname = font_path).get_name()\n",
    "rc('font', family = font)\n",
    "\n",
    "length = len(answers_df)\n",
    "for idx, value in enumerate(random.sample(range(length), 10)):\n",
    "    img_array = np.fromfile(os.path.join(custom_dataset_path,test_df.iloc[value][\"image_path\"]), np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.subplot(length,1,idx+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Q: {test_df.iloc[value][\"question\"]}     A: {answers_df.iloc[value][\"answer\"]}')\n",
    "    plt.axis('off')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BEiT-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
